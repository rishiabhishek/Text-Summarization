{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import operator\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_reviews():\n",
    "    reviews = pd.read_csv(\"../Datasets/Reviews/Reviews.csv\")\n",
    "    reviews = reviews.dropna()\n",
    "    reviews = reviews.drop([\"Id\",\"ProductId\",\"UserId\",\"ProfileName\",\"HelpfulnessNumerator\",\"HelpfulnessDenominator\",\"Score\",\"Time\"]\n",
    "                 ,axis=1)\n",
    "    return reviews\n",
    "\n",
    "reviews = read_reviews()\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Summary, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[reviews.isnull().any(axis=1)] # All cells have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning and Normalizing the text and summaries\n",
    "# Some contraction to expansion\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "def normalization(review,remove_stopwords=False):\n",
    "    text = review.lower()\n",
    "    clean_text = []\n",
    "    for word in text.split():\n",
    "        if word in contractions:\n",
    "            clean_text.append(contractions[word])\n",
    "        else:\n",
    "            clean_text.append(word)\n",
    "    text = \" \".join(clean_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "#     text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'https', ' ', text)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br', ' ', text)\n",
    "    text = re.sub(r'/>', ' ', text)\n",
    "    text = re.sub(r'>', ' ', text)\n",
    "    text = re.sub(r'<', ' ', text)\n",
    "    text = re.sub(r'`', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   http   www amazon com gp product b000gwlugu  plocky s tortilla chips  red beans  n rice  7 ounce bag  pack of 12   a  i first tasted these chips while visiting relatives in ky  they are not available where i live  so i ordered them from amazon  wow  my friends and family are all addicted to them  the spicy flavor grabs you at the first bite  once a bag is open  it is gone '"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization(reviews.Text[713])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_reviews(texts):\n",
    "    return [normalization(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = clean_reviews(reviews.Summary)\n",
    "text = clean_reviews(reviews.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None count in Summary  0\n",
      "None count in Text  0\n",
      "568412 568412\n"
     ]
    }
   ],
   "source": [
    "print(\"None count in Summary \",sum(x is None for x in summary))\n",
    "print(\"None count in Text \",sum(x is None for x in text))\n",
    "print(len(summary),len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counting the words in Text and summary and remove words having count less than threshold\n",
    "def get_word_count(texts,summaries,threshold=20):\n",
    "    '''\n",
    "    Params: Tests , Summaries ,threshold = 20\n",
    "    Return : word count dict\n",
    "    '''\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        tokens.extend(text.split())\n",
    "    for summary in summaries:\n",
    "        tokens.extend(summary.split())\n",
    "    counts = Counter(tokens)\n",
    "    reduced_count = {word:i for word,i in counts.items() if i >= threshold}\n",
    "    return reduced_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "count = get_word_count(text,summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'woof': 241,\n",
       " 'tired': 3417,\n",
       " 'gulped': 128,\n",
       " 'reddenbacher': 35,\n",
       " 'fireworks': 52,\n",
       " 'kay': 174,\n",
       " 'reducer': 29,\n",
       " 'hating': 80,\n",
       " 'sparingly': 625,\n",
       " 'cappuccinos': 224,\n",
       " 'marigold': 24,\n",
       " 'super': 13263,\n",
       " 'waaay': 67,\n",
       " 'sause': 112,\n",
       " 'apocalypse': 55,\n",
       " 'quarts': 432,\n",
       " 'terrifically': 22,\n",
       " 'blasted': 34,\n",
       " 'vitality': 525,\n",
       " 'ly': 61,\n",
       " 'bodied': 3243,\n",
       " 'horrors': 47,\n",
       " 'laxatives': 71,\n",
       " 'enhances': 618,\n",
       " 'condiment': 618,\n",
       " 'jel': 53,\n",
       " 'rockies': 27,\n",
       " 'overhead': 43,\n",
       " 'salesman': 29,\n",
       " 'crystalized': 104,\n",
       " 'eh': 410,\n",
       " 'mom': 5600,\n",
       " 'sri': 199,\n",
       " 'twitch': 40,\n",
       " '49': 781,\n",
       " 'steep': 2839,\n",
       " '~10': 25,\n",
       " 'disposakups': 69,\n",
       " 'discription': 73,\n",
       " 'dedicated': 369,\n",
       " 'coyotes': 53,\n",
       " 'uncontrollably': 28,\n",
       " 'sufferers': 96,\n",
       " 'snappy': 199,\n",
       " 'derived': 503,\n",
       " 'cleverly': 52,\n",
       " 'advocates': 28,\n",
       " 'drainage': 49,\n",
       " 'agriculture': 52,\n",
       " 'acidic': 2269,\n",
       " 'replenished': 36,\n",
       " 'si': 51,\n",
       " 'contest': 145,\n",
       " 'dissatisfied': 275,\n",
       " '410': 40,\n",
       " 'potatoe': 260,\n",
       " 'exploring': 146,\n",
       " 'informed': 617,\n",
       " 'daisy': 127,\n",
       " 'neutered': 68,\n",
       " 'teacups': 23,\n",
       " 'russa': 42,\n",
       " 'faecium': 74,\n",
       " 'frustration': 463,\n",
       " 'changed': 6017,\n",
       " 'lengthy': 90,\n",
       " 'swheat': 113,\n",
       " 'marinade': 1323,\n",
       " 'fondant': 770,\n",
       " 'pineapple': 2110,\n",
       " 'brothers': 406,\n",
       " 'enrolled': 24,\n",
       " 'chiou': 46,\n",
       " 'nonfat': 259,\n",
       " 'concoct': 26,\n",
       " 'ons': 29,\n",
       " 'bee': 485,\n",
       " 'maids': 96,\n",
       " 'tlc': 100,\n",
       " 'lies': 302,\n",
       " 'although': 14455,\n",
       " 'em': 3159,\n",
       " 'debatable': 24,\n",
       " 'vis': 33,\n",
       " 'buyers': 611,\n",
       " 'te': 69,\n",
       " 'prize': 218,\n",
       " 'remedied': 86,\n",
       " 'asthmas': 25,\n",
       " 'got': 43201,\n",
       " 'crawled': 26,\n",
       " 'bullies': 72,\n",
       " 'remaining': 1495,\n",
       " 'diagnostic': 27,\n",
       " 'deceased': 62,\n",
       " 'soo': 502,\n",
       " '6lb': 66,\n",
       " 'endearing': 26,\n",
       " 'properly': 2243,\n",
       " 'replies': 25,\n",
       " 'dehydration': 159,\n",
       " 'mile': 666,\n",
       " 'responded': 482,\n",
       " 'sleek': 177,\n",
       " 'poodles': 225,\n",
       " 'silicone': 175,\n",
       " 'styrofoam': 555,\n",
       " 'stocked': 1194,\n",
       " '187': 22,\n",
       " 'defrosted': 61,\n",
       " 'beyond': 2729,\n",
       " 'bingo': 114,\n",
       " 'comparably': 49,\n",
       " 'him': 25766,\n",
       " 'nutrious': 36,\n",
       " 'tracks': 95,\n",
       " 'geat': 69,\n",
       " 'knot': 106,\n",
       " 'forward': 4788,\n",
       " 'producer': 209,\n",
       " 'uht': 26,\n",
       " 'copious': 78,\n",
       " 'grainier': 31,\n",
       " 'beach': 835,\n",
       " 'selfish': 40,\n",
       " 'carryon': 24,\n",
       " '1c': 42,\n",
       " 'teachers': 96,\n",
       " 'chipolte': 70,\n",
       " 'shiba': 138,\n",
       " '15oz': 66,\n",
       " 'spiciness': 594,\n",
       " 'afterwards': 1047,\n",
       " 'chipotle': 1261,\n",
       " '155': 59,\n",
       " 'severely': 460,\n",
       " 'sang': 26,\n",
       " 'pocky': 175,\n",
       " 'holland': 128,\n",
       " 'antonio': 73,\n",
       " 'aerogarden': 398,\n",
       " 'chews': 5540,\n",
       " 'branches': 149,\n",
       " 'wicked': 166,\n",
       " 'wad': 83,\n",
       " 'overprocessed': 33,\n",
       " 'reporting': 55,\n",
       " 'extensively': 131,\n",
       " 'gained': 601,\n",
       " 'unbearably': 48,\n",
       " 'insures': 21,\n",
       " 'oc': 30,\n",
       " 'fashioned': 1114,\n",
       " 'ionized': 50,\n",
       " 'refuses': 645,\n",
       " 'actor': 37,\n",
       " 'rasins': 41,\n",
       " 'absence': 165,\n",
       " 'lucid': 30,\n",
       " 'misgivings': 21,\n",
       " 'acheive': 20,\n",
       " 'flock': 26,\n",
       " 'doxie': 114,\n",
       " 'papery': 61,\n",
       " 'floats': 111,\n",
       " 'dragging': 186,\n",
       " 'boxing': 60,\n",
       " 'government': 240,\n",
       " 'outdated': 234,\n",
       " 'quest': 539,\n",
       " 'floury': 32,\n",
       " 'metal': 1077,\n",
       " 'soldiers': 160,\n",
       " 'brother': 1479,\n",
       " 'sensitive': 5220,\n",
       " 'dared': 38,\n",
       " 'mins': 995,\n",
       " 'confident': 558,\n",
       " 'heed': 129,\n",
       " 'phosphoric': 85,\n",
       " 'interfering': 24,\n",
       " 'crunching': 187,\n",
       " '2015': 31,\n",
       " 'pacifier': 28,\n",
       " 'fukushima': 29,\n",
       " 'savory': 1692,\n",
       " 'midday': 139,\n",
       " 'doomhammer': 37,\n",
       " 'certain': 2757,\n",
       " 'upping': 52,\n",
       " 'winco': 50,\n",
       " 'lebanon': 28,\n",
       " 'hopelessly': 46,\n",
       " 'ey': 89,\n",
       " 'season': 2184,\n",
       " 'quibble': 132,\n",
       " 'manx': 26,\n",
       " 'lable': 71,\n",
       " 'colman': 64,\n",
       " 'repackage': 97,\n",
       " 'julia': 99,\n",
       " 'overpowered': 366,\n",
       " 'mere': 309,\n",
       " 'updated': 423,\n",
       " 'reggie': 55,\n",
       " 'relaxant': 31,\n",
       " 'counter': 2370,\n",
       " 'sponsor': 29,\n",
       " 'listed': 5048,\n",
       " 'repack': 20,\n",
       " 'shaping': 32,\n",
       " 'evenflo': 30,\n",
       " 'dogfoodadvisor': 58,\n",
       " 'vertically': 45,\n",
       " 'greyhounds': 125,\n",
       " 'ne': 97,\n",
       " 'evenly': 571,\n",
       " 'silica': 109,\n",
       " 'tbls': 70,\n",
       " 'acetic': 60,\n",
       " 'curiosity': 392,\n",
       " 'instinct': 427,\n",
       " '17g': 94,\n",
       " 'nutmeg': 730,\n",
       " 'reassess': 21,\n",
       " 'triscuit': 61,\n",
       " 'snicker': 47,\n",
       " 'loathe': 59,\n",
       " 'ignores': 88,\n",
       " 'finn': 74,\n",
       " 'shortest': 33,\n",
       " 'speak': 1149,\n",
       " 'liquer': 27,\n",
       " 'dr': 3081,\n",
       " 'tooths': 25,\n",
       " 'dismissed': 39,\n",
       " 'starches': 132,\n",
       " 'th': 149,\n",
       " 'panko': 165,\n",
       " 'impresses': 45,\n",
       " 'great': 240279,\n",
       " 'respiratory': 118,\n",
       " 'premeasured': 107,\n",
       " 'aloha': 184,\n",
       " 'dyed': 132,\n",
       " 'blockages': 51,\n",
       " '86': 169,\n",
       " 'cakester': 26,\n",
       " 'tabs': 231,\n",
       " 'dispensing': 326,\n",
       " 'highs': 34,\n",
       " 'december': 721,\n",
       " 'appetizers': 225,\n",
       " 'reasearch': 24,\n",
       " 'lifesavers': 202,\n",
       " 'cadet': 107,\n",
       " 'money': 20050,\n",
       " '199': 39,\n",
       " 'beetles': 48,\n",
       " 'member': 1070,\n",
       " 'acquiring': 48,\n",
       " 'heretofore': 20,\n",
       " 'formaldehyde': 53,\n",
       " 'seventies': 31,\n",
       " 'eyesight': 26,\n",
       " 'mortar': 340,\n",
       " 'digestable': 72,\n",
       " 'film': 822,\n",
       " 'tetley': 288,\n",
       " 'naked': 686,\n",
       " 'maniac': 34,\n",
       " 'deen': 68,\n",
       " 'steals': 77,\n",
       " 'mangoes': 350,\n",
       " 'vat': 50,\n",
       " 'colorless': 30,\n",
       " 'solomon': 145,\n",
       " 'weber': 81,\n",
       " 'plain': 10171,\n",
       " 'eas': 48,\n",
       " 'endured': 29,\n",
       " 'parve': 58,\n",
       " 'detroit': 62,\n",
       " 'morton': 128,\n",
       " 'fantasticly': 41,\n",
       " 'sriracha': 625,\n",
       " 'expiration': 4749,\n",
       " 'frown': 27,\n",
       " 'beta': 152,\n",
       " 'licensed': 42,\n",
       " 'cafeteria': 204,\n",
       " 'performance': 523,\n",
       " 'extraordinary': 293,\n",
       " 'surge': 66,\n",
       " 'tempered': 86,\n",
       " 'stuff': 45541,\n",
       " 'thickly': 61,\n",
       " 'exponentially': 29,\n",
       " 'delia': 33,\n",
       " 'facebook': 220,\n",
       " 'uterus': 264,\n",
       " 'turbo': 26,\n",
       " 'steven': 28,\n",
       " 'huckleberries': 29,\n",
       " 'terrifying': 30,\n",
       " 'anxiety': 703,\n",
       " 'consequences': 124,\n",
       " 'invented': 343,\n",
       " 'tetra': 341,\n",
       " 'tots': 231,\n",
       " 'join': 244,\n",
       " 'xl': 41,\n",
       " 'mastered': 110,\n",
       " 'boundaries': 22,\n",
       " 'christmas': 6767,\n",
       " 'accumulated': 38,\n",
       " 'misting': 23,\n",
       " 'worries': 619,\n",
       " 'dosage': 357,\n",
       " 'posted': 1077,\n",
       " 'fly': 711,\n",
       " 'twizzlers': 383,\n",
       " 'expect': 7849,\n",
       " 'pricer': 30,\n",
       " 'favorable': 334,\n",
       " 'offputting': 43,\n",
       " 'cotton': 708,\n",
       " 'decade': 294,\n",
       " 'met': 1158,\n",
       " 'serenity': 40,\n",
       " 'await': 63,\n",
       " 'rudy': 70,\n",
       " 'projectile': 46,\n",
       " 'guards': 21,\n",
       " 'terro': 36,\n",
       " 'peterson': 392,\n",
       " 'mono': 191,\n",
       " 'sanity': 115,\n",
       " 'clear': 5699,\n",
       " 'brazil': 613,\n",
       " 'confirm': 305,\n",
       " 'hundreds': 551,\n",
       " 'emril': 20,\n",
       " 'picked': 3565,\n",
       " 'waft': 67,\n",
       " 'kv': 20,\n",
       " 'fixes': 114,\n",
       " 'sprig': 27,\n",
       " 'fog': 690,\n",
       " 'middle': 3380,\n",
       " 'ella': 443,\n",
       " 'shinny': 82,\n",
       " 'fla': 23,\n",
       " 'supervisor': 40,\n",
       " '9x13': 28,\n",
       " 'nevertheless': 432,\n",
       " 'steak': 2530,\n",
       " 'sincere': 41,\n",
       " 'united': 686,\n",
       " 'daniel': 37,\n",
       " 'lager': 38,\n",
       " 'dominated': 119,\n",
       " 'dispenser': 868,\n",
       " 'leukemia': 35,\n",
       " 'bass': 45,\n",
       " 'smelliest': 21,\n",
       " 'ginormous': 46,\n",
       " 'followup': 20,\n",
       " 'allergenic': 111,\n",
       " 'dynasty': 92,\n",
       " 'pinto': 189,\n",
       " 'searches': 79,\n",
       " 'fan': 13020,\n",
       " 'hinted': 41,\n",
       " 'seals': 522,\n",
       " 'defatted': 37,\n",
       " 'lamps': 26,\n",
       " 'express': 508,\n",
       " 'photographs': 26,\n",
       " 'dublin': 78,\n",
       " 'using': 31746,\n",
       " 'pintos': 32,\n",
       " 'introducing': 244,\n",
       " 'tang': 836,\n",
       " 'staying': 701,\n",
       " 'queens': 58,\n",
       " 'chestnut': 114,\n",
       " 'missouri': 67,\n",
       " 'agent': 378,\n",
       " 'yorkies': 354,\n",
       " '234': 20,\n",
       " 'straw': 714,\n",
       " 'adaptation': 20,\n",
       " 'illegal': 193,\n",
       " 'lower': 6486,\n",
       " 'anticipated': 482,\n",
       " 'b001ell68y': 33,\n",
       " 'space': 2021,\n",
       " 'ginkgo': 63,\n",
       " 'crafting': 26,\n",
       " 'lurpak': 31,\n",
       " 'hextra': 203,\n",
       " 'someones': 35,\n",
       " 'st': 774,\n",
       " 'miki': 42,\n",
       " 'idiot': 125,\n",
       " 'willingness': 29,\n",
       " 'zipper': 219,\n",
       " 'lychee': 198,\n",
       " 'views': 58,\n",
       " 'persisted': 29,\n",
       " 'cheri': 39,\n",
       " 'viral': 67,\n",
       " 'ordering': 13650,\n",
       " 'overpower': 558,\n",
       " 'window': 489,\n",
       " 'customizable': 30,\n",
       " 'campaign': 112,\n",
       " 'what': 88990,\n",
       " 'tending': 29,\n",
       " 'carries': 2290,\n",
       " 'overpowers': 380,\n",
       " 'make': 73659,\n",
       " 'afficianado': 26,\n",
       " 'dislikes': 200,\n",
       " 'shelves': 1687,\n",
       " 'careful': 4215,\n",
       " 'americana': 27,\n",
       " 'parties': 732,\n",
       " 'fro': 66,\n",
       " 'setter': 46,\n",
       " 'nest': 72,\n",
       " 'emitted': 44,\n",
       " 'kenyan': 133,\n",
       " 'bisto': 82,\n",
       " 'poles': 20,\n",
       " 'ingedients': 32,\n",
       " 'mange': 30,\n",
       " 'fundamental': 22,\n",
       " 'politics': 25,\n",
       " 'designated': 70,\n",
       " 'superstore': 41,\n",
       " 'cleveland': 44,\n",
       " 'chard': 33,\n",
       " 'smiled': 69,\n",
       " 'palpitations': 163,\n",
       " 'seduced': 21,\n",
       " 'b': 4844,\n",
       " 'comparing': 947,\n",
       " 'marshall': 120,\n",
       " 'marbella': 23,\n",
       " 'monophosphate': 34,\n",
       " 'nonetheless': 620,\n",
       " 'territorial': 28,\n",
       " 'breaking': 1260,\n",
       " 'encounter': 177,\n",
       " '6pk': 25,\n",
       " 'lipil': 44,\n",
       " 'toasts': 79,\n",
       " 'colonial': 23,\n",
       " 'bin': 467,\n",
       " 'highlander': 63,\n",
       " 'seniors': 130,\n",
       " 'hmm': 310,\n",
       " 'increments': 29,\n",
       " 'contributed': 132,\n",
       " 'ponder': 34,\n",
       " 'cow': 969,\n",
       " 'foward': 25,\n",
       " 'beans': 17712,\n",
       " 'flawless': 86,\n",
       " 'cinamon': 52,\n",
       " 'decant': 34,\n",
       " 'chickory': 52,\n",
       " 'mths': 23,\n",
       " 'gophers': 195,\n",
       " 'captain': 165,\n",
       " 'garibaldi': 25,\n",
       " 'jewel': 194,\n",
       " 'organo': 58,\n",
       " 'accepts': 68,\n",
       " 'administer': 104,\n",
       " 'minded': 132,\n",
       " 'cyanide': 29,\n",
       " 'ex': 284,\n",
       " 'zap': 100,\n",
       " 'mores': 198,\n",
       " 'hotter': 921,\n",
       " 'folgers': 1485,\n",
       " 'dine': 45,\n",
       " 'plays': 428,\n",
       " 'bones': 5632,\n",
       " 'ahead': 2189,\n",
       " 'nursed': 38,\n",
       " 'liquid': 7071,\n",
       " 'ital': 45,\n",
       " 'piqued': 42,\n",
       " 'hojicha': 26,\n",
       " '330': 59,\n",
       " '1x': 46,\n",
       " 'dissolve': 1937,\n",
       " 'flour': 9945,\n",
       " 'jersey': 244,\n",
       " 'specialties': 64,\n",
       " 'fake': 2602,\n",
       " 'midnight': 569,\n",
       " 'bodywash': 31,\n",
       " 'organize': 40,\n",
       " 'underweight': 169,\n",
       " 'emily': 29,\n",
       " 'bichons': 55,\n",
       " 'lined': 452,\n",
       " 'dominican': 47,\n",
       " 'unforgettable': 26,\n",
       " 'peruvian': 112,\n",
       " 'meter': 72,\n",
       " 'advises': 49,\n",
       " 'scouring': 47,\n",
       " 'evens': 38,\n",
       " 'confirms': 56,\n",
       " 'spaniels': 119,\n",
       " 'further': 2206,\n",
       " 'origional': 37,\n",
       " 'bribed': 22,\n",
       " 'console': 26,\n",
       " 'marketable': 21,\n",
       " 'blessings': 60,\n",
       " 'peanutty': 179,\n",
       " 'graduation': 48,\n",
       " '270': 54,\n",
       " 'resistance': 180,\n",
       " 'ok': 13695,\n",
       " 'spiders': 75,\n",
       " 'countertops': 23,\n",
       " 'page': 1628,\n",
       " '2012': 2373,\n",
       " 'crabmeat': 88,\n",
       " 'multitude': 120,\n",
       " 'nuanced': 67,\n",
       " 'perfectly': 5705,\n",
       " 'lounge': 50,\n",
       " 'richer': 1153,\n",
       " 'doors': 145,\n",
       " 'candybar': 59,\n",
       " 'hopefully': 2299,\n",
       " 'bunches': 241,\n",
       " 'levels': 2815,\n",
       " 'empties': 81,\n",
       " 'doubleshot': 63,\n",
       " 'goldenseal': 31,\n",
       " 'restock': 157,\n",
       " 'snuggly': 28,\n",
       " 'litre': 34,\n",
       " 'ketogenic': 28,\n",
       " 'smoothe': 85,\n",
       " 'demon': 24,\n",
       " 'flatulence': 137,\n",
       " 'minature': 59,\n",
       " 'spritzer': 54,\n",
       " 'lickety': 774,\n",
       " 'fru': 41,\n",
       " 'bhaji': 29,\n",
       " 'chapped': 37,\n",
       " 'retailer': 550,\n",
       " 'especial': 22,\n",
       " 'casbah': 43,\n",
       " 'splashed': 34,\n",
       " 'scooter': 32,\n",
       " 'testy': 32,\n",
       " 'failed': 805,\n",
       " 'margin': 196,\n",
       " 'thai': 3295,\n",
       " 'spoonable': 23,\n",
       " 'depended': 41,\n",
       " 'lettering': 60,\n",
       " 'inconvenient': 271,\n",
       " 'thinner': 975,\n",
       " 'introductory': 25,\n",
       " 'insoluble': 85,\n",
       " 'argan': 44,\n",
       " 'taster': 402,\n",
       " 'group': 1090,\n",
       " 'gyros': 21,\n",
       " 'cella': 31,\n",
       " 'doctoring': 121,\n",
       " 'unsure': 449,\n",
       " 'pea': 1082,\n",
       " 'alarm': 139,\n",
       " 'proverbial': 35,\n",
       " 'lingers': 481,\n",
       " 'dabur': 24,\n",
       " 'policy': 584,\n",
       " 'wallet': 368,\n",
       " 'unnecessary': 617,\n",
       " 'offerings': 682,\n",
       " 'jennies': 23,\n",
       " 'legume': 23,\n",
       " 'cbd': 57,\n",
       " 'booth': 44,\n",
       " 'grittiness': 116,\n",
       " 'disillusioned': 22,\n",
       " 'definintely': 21,\n",
       " 'sunday': 549,\n",
       " 'sandwich': 2957,\n",
       " 'n': 4466,\n",
       " 'geek': 41,\n",
       " 'favors': 676,\n",
       " 'luxury': 413,\n",
       " 'mitchell': 42,\n",
       " 'helen': 24,\n",
       " 'charged': 672,\n",
       " 'driven': 174,\n",
       " 'betty': 814,\n",
       " 'reviews': 19576,\n",
       " 'pad': 617,\n",
       " 'splatter': 45,\n",
       " 'jean': 1238,\n",
       " 'wuyi': 61,\n",
       " 'particulary': 34,\n",
       " 'contaminants': 96,\n",
       " 'fallot': 20,\n",
       " 'halal': 94,\n",
       " '~amy': 37,\n",
       " 'smitten': 29,\n",
       " 'pepperoni': 708,\n",
       " 'nevermind': 31,\n",
       " 'october': 592,\n",
       " 'starchy': 215,\n",
       " 'palm': 1408,\n",
       " 'tried': 77687,\n",
       " 'hubby': 1287,\n",
       " 'thick': 7289,\n",
       " 'rarity': 83,\n",
       " 'pizzaz': 31,\n",
       " 'drivers': 28,\n",
       " 'mercedes': 22,\n",
       " 'comin': 39,\n",
       " 'ammount': 45,\n",
       " 'obligate': 112,\n",
       " 'friend': 9258,\n",
       " '420': 53,\n",
       " 'softened': 257,\n",
       " 'approach': 467,\n",
       " 'concave': 28,\n",
       " 'sophisticated': 383,\n",
       " 'asin': 310,\n",
       " 'art': 505,\n",
       " 'giver': 41,\n",
       " 'vials': 73,\n",
       " 'enjoyed': 10398,\n",
       " 'yacon': 48,\n",
       " 'stabilizing': 33,\n",
       " 'voices': 30,\n",
       " 'spagetti': 89,\n",
       " 'usefulness': 44,\n",
       " 'ices': 20,\n",
       " 'colon': 237,\n",
       " 'stuffers': 210,\n",
       " 'woody': 220,\n",
       " 'sneeze': 76,\n",
       " 'interstate': 20,\n",
       " 'undeniable': 32,\n",
       " 'coats': 1563,\n",
       " 'esque': 77,\n",
       " 'pill': 5607,\n",
       " 'compostable': 145,\n",
       " 'terrier': 1537,\n",
       " 'fads': 38,\n",
       " 'weakly': 31,\n",
       " 'swept': 66,\n",
       " 'pawn': 71,\n",
       " 'mashups': 23,\n",
       " 'harissa': 93,\n",
       " 'raccoon': 64,\n",
       " 'transfat': 65,\n",
       " 'indulging': 215,\n",
       " 'hears': 268,\n",
       " 'blob': 150,\n",
       " 'foods': 25404,\n",
       " 'chocoperfection': 26,\n",
       " 'personal': 3608,\n",
       " '1519': 44,\n",
       " 'barry': 519,\n",
       " 'listens': 37,\n",
       " 'yards': 51,\n",
       " 'physiology': 35,\n",
       " 'eaten': 7823,\n",
       " 'supplying': 84,\n",
       " '3g': 660,\n",
       " 'fibre': 92,\n",
       " 'puzzled': 149,\n",
       " 'coincidentally': 42,\n",
       " '35': 2054,\n",
       " 'dissipated': 36,\n",
       " 'basis': 2970,\n",
       " 'success': 1696,\n",
       " 'these': 247317,\n",
       " 'yoga': 243,\n",
       " 'yummier': 79,\n",
       " 'radiation': 119,\n",
       " 'meaningless': 37,\n",
       " 'reads': 394,\n",
       " 'lifted': 132,\n",
       " 'bud': 394,\n",
       " 'oxidized': 171,\n",
       " 'spoilage': 125,\n",
       " 'making': 18171,\n",
       " 'orijen': 1101,\n",
       " 'garbanzo': 237,\n",
       " 'indonesian': 102,\n",
       " 'land': 560,\n",
       " 'messiness': 26,\n",
       " 'accompanying': 91,\n",
       " 'berbere': 26,\n",
       " 'cassis': 27,\n",
       " 'pekingese': 70,\n",
       " 'tnr': 20,\n",
       " 'spewed': 46,\n",
       " 'scary': 470,\n",
       " 'feedback': 707,\n",
       " 'coaxed': 32,\n",
       " 'crisped': 62,\n",
       " 'injection': 40,\n",
       " 'pulse': 73,\n",
       " 'able': 17288,\n",
       " 'dog': 83544,\n",
       " 'trailer': 54,\n",
       " 'identified': 200,\n",
       " 'mar': 75,\n",
       " 'hashbrowns': 45,\n",
       " 'relieving': 94,\n",
       " 'bust': 174,\n",
       " 'ch': 29,\n",
       " 'spitz': 63,\n",
       " 'emergen': 120,\n",
       " 'variable': 117,\n",
       " 'fuji': 139,\n",
       " 'parsely': 28,\n",
       " 'maddy': 21,\n",
       " 'pardon': 43,\n",
       " 'combining': 326,\n",
       " 'accustomed': 640,\n",
       " 'carry': 7076,\n",
       " 'aid': 1999,\n",
       " 'sealed': 4656,\n",
       " 'leftovers': 1083,\n",
       " 'dollop': 283,\n",
       " 'stretch': 629,\n",
       " 'niblets': 24,\n",
       " 'spruce': 47,\n",
       " 'kups': 143,\n",
       " 'centerpieces': 20,\n",
       " 'mentioned': 3839,\n",
       " 'stephen': 244,\n",
       " 'oat': 2146,\n",
       " 'misshapen': 61,\n",
       " '2lbs': 86,\n",
       " 'glutened': 28,\n",
       " 'ww': 292,\n",
       " 'durable': 943,\n",
       " 'scorned': 20,\n",
       " 'stabilized': 60,\n",
       " 'whey': 1482,\n",
       " 'pooh': 44,\n",
       " 'kix': 89,\n",
       " 'downer': 58,\n",
       " 'ambulance': 24,\n",
       " 'troublesome': 41,\n",
       " 'gamut': 29,\n",
       " 'branching': 42,\n",
       " 'knocking': 585,\n",
       " 'bench': 46,\n",
       " 'claims': 2451,\n",
       " 'chances': 483,\n",
       " 'accented': 37,\n",
       " 'wholesome': 1793,\n",
       " 'generally': 3883,\n",
       " 'far': 29934,\n",
       " 'robustness': 38,\n",
       " 'chianti': 38,\n",
       " 'houseplants': 41,\n",
       " 'shrimps': 32,\n",
       " 'effortlessly': 47,\n",
       " 'loco': 38,\n",
       " 'petal': 66,\n",
       " 'overload': 230,\n",
       " 'pesky': 96,\n",
       " 'sounding': 149,\n",
       " 'soya': 76,\n",
       " 'steam': 867,\n",
       " 'archie': 46,\n",
       " 'often': 10653,\n",
       " 'bottle': 21155,\n",
       " 'boots': 44,\n",
       " 'zillion': 55,\n",
       " 'lure': 122,\n",
       " 'elderberry': 115,\n",
       " 'bodies': 470,\n",
       " 'parmesean': 48,\n",
       " 'lapsed': 24,\n",
       " 'breads': 1272,\n",
       " 'tecture': 22,\n",
       " 'burlap': 72,\n",
       " 'preheated': 49,\n",
       " 'hormones': 345,\n",
       " 'nightly': 217,\n",
       " 'pukes': 34,\n",
       " 'clumpy': 219,\n",
       " 'cvs': 176,\n",
       " 'ect': 175,\n",
       " 'taking': 7284,\n",
       " 'glen': 290,\n",
       " 'cantaloupe': 42,\n",
       " 'upload': 58,\n",
       " 'threat': 71,\n",
       " 'trap': 1937,\n",
       " 'turning': 875,\n",
       " 'owner': 1509,\n",
       " 'irradiated': 109,\n",
       " 'maximize': 76,\n",
       " 'recreating': 21,\n",
       " 'mmmmmmmmm': 48,\n",
       " 'otis': 54,\n",
       " 'coffes': 26,\n",
       " '2500': 53,\n",
       " 'influence': 135,\n",
       " 'relieved': 363,\n",
       " 'investing': 135,\n",
       " 'missing': 3034,\n",
       " 'guys': 1855,\n",
       " 'profound': 52,\n",
       " 'saltiness': 580,\n",
       " 'vitacost': 136,\n",
       " 'crumby': 67,\n",
       " 'approximately': 1522,\n",
       " 'props': 70,\n",
       " 'brunch': 103,\n",
       " 'denta': 41,\n",
       " '2011': 1988,\n",
       " 'seitenbacher': 50,\n",
       " 'labors': 59,\n",
       " 'steel': 1551,\n",
       " 'schnoodle': 55,\n",
       " 'static': 38,\n",
       " 'pretzel': 1482,\n",
       " 'biologically': 32,\n",
       " '1000mg': 33,\n",
       " 'assimilate': 28,\n",
       " 'bucket': 552,\n",
       " 'serious': 2325,\n",
       " 'expelled': 23,\n",
       " 'fluctuates': 80,\n",
       " 'reheating': 109,\n",
       " 'cringing': 21,\n",
       " 'references': 70,\n",
       " 'inka': 46,\n",
       " 'tropics': 72,\n",
       " 'awards': 69,\n",
       " 'concur': 98,\n",
       " 'fatty': 1300,\n",
       " 'mozzerella': 25,\n",
       " 'collector': 68,\n",
       " 'curiously': 79,\n",
       " 'urninary': 34,\n",
       " 'confess': 289,\n",
       " 'agencies': 68,\n",
       " 'catalogue': 26,\n",
       " '2lb': 176,\n",
       " 'premier': 217,\n",
       " 'truvia': 674,\n",
       " 'bragging': 54,\n",
       " 'calmed': 95,\n",
       " 'obscure': 82,\n",
       " 'easiest': 659,\n",
       " 'thunderstorms': 22,\n",
       " 'cruelty': 89,\n",
       " 'brushings': 24,\n",
       " 'hike': 430,\n",
       " 'commercial': 1875,\n",
       " 'imported': 913,\n",
       " 'inserts': 101,\n",
       " 'maltodextrin': 705,\n",
       " 'chamomiles': 30,\n",
       " 'touts': 82,\n",
       " 'uuml': 80,\n",
       " 'allegro': 46,\n",
       " 'oily': 2638,\n",
       " 'smaller': 10479,\n",
       " 'dino': 51,\n",
       " 'summertime': 198,\n",
       " 'posters': 107,\n",
       " 'albertson': 61,\n",
       " 'locust': 25,\n",
       " 'obtrusive': 25,\n",
       " 'bischon': 24,\n",
       " 'defeating': 27,\n",
       " 'oakland': 37,\n",
       " 'conclude': 131,\n",
       " 'pioneer': 26,\n",
       " 'puked': 56,\n",
       " 'skinned': 34,\n",
       " 'sauces': 4012,\n",
       " 'irrelevant': 60,\n",
       " 'gas': 4512,\n",
       " 'turnover': 39,\n",
       " 'amaranth': 318,\n",
       " 'shopped': 333,\n",
       " 'emperor': 39,\n",
       " 'maintain': 1361,\n",
       " 'shade': 311,\n",
       " 'improvise': 62,\n",
       " 'calmer': 77,\n",
       " 'runny': 1252,\n",
       " 'ban': 100,\n",
       " '10am': 24,\n",
       " 'tragically': 23,\n",
       " 'poked': 30,\n",
       " 'police': 82,\n",
       " 'flossie': 43,\n",
       " 'assisted': 32,\n",
       " 'zealand': 374,\n",
       " 'dismal': 30,\n",
       " 'chute': 22,\n",
       " 'kreme': 29,\n",
       " 'vodka': 778,\n",
       " 'bisquik': 28,\n",
       " 'saturday': 453,\n",
       " 'snakes': 30,\n",
       " 'tale': 131,\n",
       " 'trivial': 45,\n",
       " 'leech': 39,\n",
       " 'tiramisu': 147,\n",
       " 'closing': 229,\n",
       " '~~~~~~~~~~~~~~~~~~': 61,\n",
       " 'along': 8576,\n",
       " 'frutti': 34,\n",
       " 'purist': 203,\n",
       " 'boycott': 61,\n",
       " 'revered': 30,\n",
       " 'attaches': 23,\n",
       " 'fire': 1134,\n",
       " 'usps': 338,\n",
       " 'b00474or8g': 20,\n",
       " 'blatantly': 31,\n",
       " 'reishi': 28,\n",
       " 'baronet': 221,\n",
       " 'jammin': 24,\n",
       " 'duane': 39,\n",
       " 'panama': 70,\n",
       " 'acres': 82,\n",
       " 'couldn': 210,\n",
       " 'rasp': 57,\n",
       " 'plumped': 25,\n",
       " 'tylenol': 68,\n",
       " 'sniffed': 435,\n",
       " 'starbuck': 895,\n",
       " 'your': 93303,\n",
       " 'instincts': 44,\n",
       " 'soothed': 62,\n",
       " 'says': 11564,\n",
       " 'boxful': 24,\n",
       " 'flies': 505,\n",
       " 'mahalo': 23,\n",
       " 'freshness': 2789,\n",
       " 'siracha': 27,\n",
       " 'deliverd': 21,\n",
       " 'bagel': 460,\n",
       " 'spewing': 43,\n",
       " 'cookie': 15090,\n",
       " 'barely': 3078,\n",
       " 'there': 91558,\n",
       " 'touched': 311,\n",
       " 'snyders': 69,\n",
       " 'kombu': 74,\n",
       " 'rugs': 72,\n",
       " 'snarf': 63,\n",
       " 'farm': 1178,\n",
       " 'stackable': 24,\n",
       " 'plastered': 30,\n",
       " 'dale': 49,\n",
       " 'chugging': 69,\n",
       " 'persistent': 85,\n",
       " 'roadhouse': 32,\n",
       " 'clippings': 108,\n",
       " 'advertised': 3327,\n",
       " 'machinery': 39,\n",
       " 'this': 699285,\n",
       " 'transfers': 24,\n",
       " 'nelson': 24,\n",
       " '5mg': 99,\n",
       " 'maille': 43,\n",
       " 'incorporation': 22,\n",
       " 'grillin': 21,\n",
       " 'demands': 144,\n",
       " 'crystals': 1198,\n",
       " 'canines': 97,\n",
       " 'kindness': 69,\n",
       " 'dollar': 2229,\n",
       " 'flight': 323,\n",
       " 'goal': 516,\n",
       " ...}"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(word_counts):\n",
    "    '''\n",
    "    Param: word_counts\n",
    "    Return: Vocab,vocab_to_int,int_to_vocab\n",
    "    '''\n",
    "    vocab = set(word_counts.keys())\n",
    "    \n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    \n",
    "    codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
    "    for i,code in enumerate(codes):\n",
    "        vocab_to_int[code] = i\n",
    "\n",
    "    for i,word in enumerate(vocab,4):\n",
    "        vocab_to_int[word] = i\n",
    "        \n",
    "    int_to_vocab = {i:word for word,i in vocab_to_int.items()}\n",
    "    return vocab,vocab_to_int,int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab,vocab_to_int,int_to_vocab = get_vocab(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22726 22730 22730\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab),len(vocab_to_int),len(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pre-trained Conceptnet Numberbatch's Embeddings (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "def get_word_embeddings():\n",
    "    embeddings = {}\n",
    "    with open('../Datasets/embeddings/numberbatch-en-17.06.txt',encoding='utf-8') as em:\n",
    "        for embed in em:\n",
    "            em_line = embed.split(' ')\n",
    "            if len(em_line) > 2: # First line of file is no. of words , number of dimensions\n",
    "                word = em_line[0]\n",
    "                embedding = np.array(em_line[1:])\n",
    "                embeddings[word] = embedding\n",
    "    print('Word embeddings:', len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417194\n"
     ]
    }
   ],
   "source": [
    "CN_embeddings = get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_in_embeddings = [word for word in vocab if word not in CN_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words not in Ebeddings :  2759\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of words not in Ebeddings : \",len(not_in_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(int_to_vocab,embeddings,embedding_dim = 300):\n",
    "    '''\n",
    "    Params : int_to_vocab, embeddings, embedding_dim\n",
    "    Return : embedding matrix\n",
    "    '''\n",
    "    # Generating empty numpy matrix\n",
    "    embeding_matrix = np.zeros([len(vocab_to_int),embedding_dim])\n",
    "    embeding_matrix = embeding_matrix.astype(np.float32)\n",
    "    \n",
    "    #Generating random embeddings for words not in CN embeddings\n",
    "    for i,word in int_to_vocab.items():\n",
    "        if word in embeddings:\n",
    "            embeding_matrix[i] = embeddings[word]\n",
    "        else:\n",
    "            embeding_matrix[i] = np.array(np.random.normal(embedding_dim))\n",
    "    return embeding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeding_matrix = create_embedding_matrix(int_to_vocab,CN_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22730 22730\n"
     ]
    }
   ],
   "source": [
    "print(len(embeding_matrix),len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_source_target(sources, targets, vocab_to_int):\n",
    "    '''\n",
    "    Params : Sources, Targets, vocab_to_int\n",
    "    Return :encoded_sources, encoded_targets\n",
    "    '''\n",
    "    encoded_sources = []\n",
    "    encoded_targets = []\n",
    "    for source in sources:\n",
    "        encod_ent = []\n",
    "        for word in source.split():\n",
    "            if word in vocab_to_int:\n",
    "                encod_ent.append(vocab_to_int[word])\n",
    "            else:\n",
    "                encod_ent.append(vocab_to_int[\"<UNK>\"])\n",
    "        encoded_sources.append(encod_ent)\n",
    "    \n",
    "    for target in targets:\n",
    "        encod_ent = []\n",
    "        for word in target.split():\n",
    "            if word in vocab_to_int:\n",
    "                encod_ent.append(vocab_to_int[word])\n",
    "            else:\n",
    "                encod_ent.append(vocab_to_int[\"<UNK>\"])\n",
    "        encoded_targets.append(encod_ent)\n",
    "        \n",
    "    return encoded_sources, encoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_sources, encoded_targets = encode_source_target(text,summary,vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568412 568412\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_sources),len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Building Input Placeholders\n",
    "def model_inputs():\n",
    "    '''\n",
    "    Returns : input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length\n",
    "    '''\n",
    "    input_ = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"inputs\")\n",
    "    target = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"target\")\n",
    "    \n",
    "    learning_rate = tf.placeholder(dtype=tf.float32,name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "    source_seq_length = tf.placeholder(dtype=tf.int32,shape=(None,),name=\"source_seq_length\")\n",
    "    target_seq_length = tf.placeholder(dtype=tf.int32,shape=(None,),name=\"target_seq_length\")\n",
    "    \n",
    "    max_target_seq_length = tf.reduce_max(target_seq_length,name=\"max_target_seq_length\")\n",
    "    return input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process decoder input\n",
    "def process_decoder_input(target_data,vocab_to_int,batch_size):\n",
    "    \n",
    "    strided_target = tf.strided_slice(target_data,(0,0),(batch_size,-1),(1,1))\n",
    "    go = tf.fill(value=vocab_to_int[\"<GO>\"],dims=(batch_size,1))\n",
    "    decoder_input = tf.concat((go,strided_target),axis=1)\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create LSTM cells\n",
    "def get_lstm(rnn_size,keep_prob=0.7):\n",
    "    lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "    drop = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "    return drop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encoding_layer(embeded_rnn_input,rnn_size,keep_prob,num_layers,batch_size,source_sequence_length):\n",
    "#     forward lstm layer\n",
    "    cell_fw = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "    initial_state_fw = cell_fw.zero_state(batch_size,dtype=tf.float32)\n",
    "#     backward lstm layer\n",
    "    cell_bw = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "    initial_state_bw =cell_bw.zero_state(batch_size,dtype=tf.float32)\n",
    "    \n",
    "    output,output_states = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,cell_bw=cell_bw,inputs=embeded_rnn_input,\n",
    "                                    sequence_length=source_sequence_length,initial_state_fw=initial_state_fw,\n",
    "                                                           initial_state_bw=initial_state_bw)\n",
    "    \n",
    "#     output = tf.cast(output,tf.float32)\n",
    "#     output=tf.concat(output,axis=2)\n",
    "    return output,output_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoder(dec_embed_input,decoder_cell,encoder_state, output_layer,\n",
    "                     target_sequence_length,max_target_length):\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input,target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    final_outputs, final_state = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,impute_finished=True,\n",
    "                                                     maximum_iterations=max_target_length)\n",
    "    \n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoder(embeddings,decoder_cell,encoder_state,output_layer,vocab_to_int,\n",
    "                      max_target_length,batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant(dtype=tf.int32,value=[vocab_to_int[\"<GO>\"]]),\n",
    "                           multiples=[batch_size],name=\"start_tokens\")\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                      start_tokens=start_tokens,\n",
    "                                                      end_token=vocab_to_int[\"<EOS>\"])\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    final_output, final_state = tf.contrib.seq2seq.dynamic_decode(decoder,impute_finished=True,\n",
    "                                                     maximum_iterations=max_target_length)\n",
    "    return final_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_inputs,encoder_state,embedding,vocab_to_int,rnn_size,target_sequence_length,max_target_length,\n",
    "                   batch_size,num_layers):\n",
    "    \n",
    "    vocab_len = len(vocab_to_int)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_len,kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    \n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embedding,target_inputs)\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "        \n",
    "        training_logits = training_decoder(embed,decoder_cell,encoder_state,output_layer,\n",
    "                                         target_sequence_length,max_target_length)\n",
    "    \n",
    "        \n",
    "    with tf.variable_scope(\"decoding\",reuse=True):\n",
    "        \n",
    "        inference_logits = inference_decoder(embeddings,decoder_cell,encoder_state,output_layer,vocab_to_int,\n",
    "                                          max_target_length,batch_size)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(source_input,target_input,embeding_matrix,vocab_to_int,source_sequence_length,\n",
    "                  target_sequence_length,max_target_length, rnn_size,keep_prob,num_layers,batch_size):\n",
    "    '''\n",
    "    Params : source_input,target_input,embeding_matrix,vocab_to_int,source_sequence_length,\n",
    "                  target_sequence_length,max_target_length, rnn_size,keep_prob,num_layers,batch_size\n",
    "    \n",
    "    Return : training_logits, inference_logits\n",
    "    '''\n",
    "    embedings = embeding_matrix\n",
    "    embed = tf.nn.embedding_lookup(embedings,source_input)\n",
    "    \n",
    "    encoder_output,encoder_states = encoding_layer(embed,rnn_size,keep_prob,num_layers,\n",
    "                                                   batch_size,source_sequence_length)\n",
    "    \n",
    "    training_logits, inference_logits = decoding_layer(target_input,encoder_states,embedings,\n",
    "                                                                vocab_to_int,rnn_size,target_sequence_length,\n",
    "                                                                max_target_length,batch_size,num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting the text and summary for better padding\n",
    "# sort based on length of length of text\n",
    "def sort_text_summary(texts,summaries):\n",
    "    text_length = [(i,text,len(text)) for i,text in enumerate(texts)]\n",
    "    text_length.sort(key=operator.itemgetter(2))\n",
    "    \n",
    "    sorted_text = [text for i,text,length in text_length]\n",
    "    sorted_summary = []\n",
    "    for i,text,length in text_length:\n",
    "        sorted_summary.append(summaries[i])\n",
    "    return sorted_text,sorted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_text, sorted_summary = sort_text_summary(encoded_sources,encoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[16197],\n",
       " [8501, 21786, 21730],\n",
       " [12920,\n",
       "  10168,\n",
       "  18195,\n",
       "  15998,\n",
       "  15998,\n",
       "  10176,\n",
       "  7866,\n",
       "  12200,\n",
       "  13947,\n",
       "  10307,\n",
       "  6704,\n",
       "  8654],\n",
       " [10977],\n",
       " [9988,\n",
       "  4122,\n",
       "  7488,\n",
       "  21695,\n",
       "  14355,\n",
       "  6603,\n",
       "  13882,\n",
       "  10453,\n",
       "  20529,\n",
       "  20879,\n",
       "  7750,\n",
       "  21503]]"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted_summary[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Padding batches\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_length = max([len(sent) for sent in sentence_batch])\n",
    "    padded_sentences = []\n",
    "    for sent in sentence_batch:\n",
    "        sent_len = len(sent)\n",
    "        if len(sent) < max_length:\n",
    "            padded_sentences.append(sent + [vocab_to_int[\"<PAD>\"] for _ in range(max_length - sent_len)])\n",
    "        else:\n",
    "            padded_sentences.append(sent)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(encoded_sources, encoded_targets, batch_size):\n",
    "    \n",
    "    '''\n",
    "    Params : encoded_sources, encoded_targets, batch_size\n",
    "    Return : text_batch,summary_batch,source_seq_len,target_seq_len\n",
    "    '''\n",
    "    \n",
    "    sorted_text, sorted_summary = sort_text_summary(encoded_sources,encoded_targets)\n",
    "    \n",
    "    batch_count = len(sorted_text)//batch_size\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        text_batch = np.array(pad_sentence_batch(sorted_text[start:end]))\n",
    "        summary_batch = np.array(pad_sentence_batch(sorted_summary[start:end]))\n",
    "        \n",
    "        source_seq_len = [len(sent) for sent in text_batch]\n",
    "        target_seq_len = [len(sent) for sent in summary_batch]\n",
    "        \n",
    "        yield (text_batch,summary_batch,source_seq_len,target_seq_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparametrs\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learning_rate = 0.01\n",
    "keep_probability = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Some cells return tuples of states, but the flag state_is_tuple is not set.  State sizes are: [LSTMStateTuple(c=256, h=256), LSTMStateTuple(c=256, h=256)]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-85ee67bc88e5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Create the training and inference logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     training_logits, inference_logits = seq2seq_model(input_,target,embeding_matrix,vocab_to_int,source_seq_length,target_seq_length,\n\u001b[0;32m---> 11\u001b[0;31m                   max_target_seq_length,rnn_size,keep_probability,num_layers,batch_size)\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;31m# Create tensors for the training logits and inference logits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-130-5ad1bf459bd7>\u001b[0m in \u001b[0;36mseq2seq_model\u001b[0;34m(source_input, target_input, embeding_matrix, vocab_to_int, source_sequence_length, target_sequence_length, max_target_length, rnn_size, keep_prob, num_layers, batch_size)\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     encoder_output,encoder_states = encoding_layer(embed,rnn_size,keep_prob,num_layers,\n\u001b[0;32m---> 13\u001b[0;31m                                                    batch_size,source_sequence_length)\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m     training_logits, inference_logits = decoding_layer(target_input,encoder_states,embedings,\n",
      "\u001b[0;32m<ipython-input-147-f940d3d47a86>\u001b[0m in \u001b[0;36mencoding_layer\u001b[0;34m(embeded_rnn_input, rnn_size, keep_prob, num_layers, batch_size, source_sequence_length)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mencoding_layer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeded_rnn_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msource_sequence_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#     forward lstm layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mcell_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMultiRNNCell\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mget_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrnn_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mkeep_prob\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mstate_is_tuple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0minitial_state_fw\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcell_fw\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_state\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat32\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m#     backward lstm layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/contrib/rnn/python/ops/core_rnn_cell_impl.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, cells, state_is_tuple)\u001b[0m\n\u001b[1;32m    911\u001b[0m         raise ValueError(\"Some cells return tuples of states, but the flag \"\n\u001b[1;32m    912\u001b[0m                          \u001b[0;34m\"state_is_tuple is not set.  State sizes are: %s\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 913\u001b[0;31m                          % str([c.state_size for c in self._cells]))\n\u001b[0m\u001b[1;32m    914\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    915\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Some cells return tuples of states, but the flag state_is_tuple is not set.  State sizes are: [LSTMStateTuple(c=256, h=256), LSTMStateTuple(c=256, h=256)]"
     ]
    }
   ],
   "source": [
    "# Build Graph\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs   \n",
    "    input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length = model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(input_,target,embeding_matrix,vocab_to_int,source_seq_length,target_seq_length,\n",
    "                  max_target_seq_length,rnn_size,keep_probability,num_layers,batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_seq_length, max_target_seq_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(inference_logits,target,masks)\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

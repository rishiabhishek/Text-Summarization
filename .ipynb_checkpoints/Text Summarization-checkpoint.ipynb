{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import operator\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_reviews():\n",
    "    reviews = pd.read_csv(\"./Datasets/Reviews/Reviews.csv\")\n",
    "    reviews = reviews.dropna()\n",
    "    reviews = reviews.drop([\"Id\",\"ProductId\",\"UserId\",\"ProfileName\",\"HelpfulnessNumerator\",\"HelpfulnessDenominator\",\"Score\",\"Time\"]\n",
    "                 ,axis=1)\n",
    "    return reviews\n",
    "\n",
    "reviews = read_reviews()\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Summary, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[reviews.isnull().any(axis=1)] # All cells have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning and Normalizing the text and summaries\n",
    "# Some contraction to expansion\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "def normalization(review,remove_stopwords=False):\n",
    "    text = review.lower()\n",
    "    clean_text = []\n",
    "    for word in text.split():\n",
    "        if word in contractions:\n",
    "            clean_text.append(contractions[word])\n",
    "        else:\n",
    "            clean_text.append(word)\n",
    "    text = \" \".join(clean_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "#     text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'https', ' ', text)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br', ' ', text)\n",
    "    text = re.sub(r'/>', ' ', text)\n",
    "    text = re.sub(r'>', ' ', text)\n",
    "    text = re.sub(r'<', ' ', text)\n",
    "    text = re.sub(r'`', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   http   www amazon com gp product b000gwlugu  plocky s tortilla chips  red beans  n rice  7 ounce bag  pack of 12   a  i first tasted these chips while visiting relatives in ky  they are not available where i live  so i ordered them from amazon  wow  my friends and family are all addicted to them  the spicy flavor grabs you at the first bite  once a bag is open  it is gone '"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization(reviews.Text[713])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_reviews(texts):\n",
    "    return [normalization(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = clean_reviews(reviews.Summary)\n",
    "text = clean_reviews(reviews.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None count in Summary  0\n",
      "None count in Text  0\n",
      "568412 568412\n"
     ]
    }
   ],
   "source": [
    "print(\"None count in Summary \",sum(x is None for x in summary))\n",
    "print(\"None count in Text \",sum(x is None for x in text))\n",
    "print(len(summary),len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counting the words in Text and summary and remove words having count less than threshold\n",
    "def get_word_count(texts,summaries,threshold=20):\n",
    "    '''\n",
    "    Params: Tests , Summaries ,threshold = 20\n",
    "    Return : word count dict\n",
    "    '''\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        tokens.extend(text.split())\n",
    "    for summary in summaries:\n",
    "        tokens.extend(summary.split())\n",
    "    counts = Counter(tokens)\n",
    "    reduced_count = {word:i for word,i in counts.items() if i >= threshold}\n",
    "    return reduced_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = get_word_count(text,summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gunk': 136,\n",
       " 'amped': 36,\n",
       " 'skipjack': 59,\n",
       " 'freaking': 272,\n",
       " 'coney': 24,\n",
       " 'discoloration': 34,\n",
       " 'muncher': 34,\n",
       " 'evaluating': 34,\n",
       " 'japanese': 1934,\n",
       " 'gunpowder': 527,\n",
       " 'occur': 257,\n",
       " 'mysteriously': 49,\n",
       " 'soaks': 124,\n",
       " 'newbie': 115,\n",
       " 'tjs': 24,\n",
       " 'nascar': 29,\n",
       " 'drought': 26,\n",
       " 'acne': 495,\n",
       " 'embarrassingly': 30,\n",
       " 'again': 55642,\n",
       " 'billed': 125,\n",
       " 'dietician': 33,\n",
       " 'carroll': 23,\n",
       " 'unpleasant': 2248,\n",
       " 'hosting': 58,\n",
       " 'colorful': 530,\n",
       " 'pounder': 105,\n",
       " 'idea': 9359,\n",
       " 'b002ievjry': 42,\n",
       " 'rate': 3724,\n",
       " 'pinwheels': 25,\n",
       " 'required': 1546,\n",
       " 'athletic': 83,\n",
       " 'sams': 397,\n",
       " 'uhm': 34,\n",
       " 'alpha': 166,\n",
       " 'bitterly': 22,\n",
       " '7yr': 21,\n",
       " 'zola': 95,\n",
       " 'lanka': 177,\n",
       " 'plumper': 37,\n",
       " 'woodsy': 113,\n",
       " 'chee': 26,\n",
       " 'marsala': 36,\n",
       " 'begins': 383,\n",
       " 'waring': 62,\n",
       " 'bubble': 1635,\n",
       " 'solixir': 26,\n",
       " 'usd': 89,\n",
       " 'tuxedo': 48,\n",
       " 'hyperactivity': 26,\n",
       " 'alternate': 602,\n",
       " 'dye': 616,\n",
       " 'gladly': 414,\n",
       " 'wax': 549,\n",
       " 'meowing': 147,\n",
       " 'internet': 2767,\n",
       " 'capful': 21,\n",
       " 'definately': 1770,\n",
       " 'respective': 48,\n",
       " 'inexpensive': 1892,\n",
       " 'omit': 83,\n",
       " 'afternoon': 4649,\n",
       " '214': 21,\n",
       " 'endurance': 234,\n",
       " 'hell': 840,\n",
       " 'devoid': 125,\n",
       " 'filtering': 89,\n",
       " 'palettes': 30,\n",
       " 'knox': 91,\n",
       " 'medicinal': 778,\n",
       " 'indications': 37,\n",
       " 'induction': 37,\n",
       " 'flavonoids': 116,\n",
       " 'folded': 179,\n",
       " 'realm': 104,\n",
       " 'bypassed': 23,\n",
       " 'liquors': 51,\n",
       " 'tagline': 21,\n",
       " 'jerkey': 442,\n",
       " 'voltage': 22,\n",
       " 'boodum': 28,\n",
       " 'accented': 37,\n",
       " 'confessions': 23,\n",
       " 'beloved': 662,\n",
       " 'perrier': 92,\n",
       " 'gogi': 22,\n",
       " 'kitchens': 344,\n",
       " 'ow': 21,\n",
       " 'angle': 186,\n",
       " 'catalog': 107,\n",
       " 'movement': 262,\n",
       " 'tasty': 37146,\n",
       " 'edited': 248,\n",
       " 'licorice': 7124,\n",
       " 'pharma': 29,\n",
       " 'raging': 47,\n",
       " 'courser': 23,\n",
       " 'hmm': 310,\n",
       " 'cheesey': 66,\n",
       " 'drum': 77,\n",
       " 'protecting': 147,\n",
       " 'clip': 313,\n",
       " 'splinter': 146,\n",
       " 'nondescript': 53,\n",
       " 'richer': 1153,\n",
       " 'drippings': 67,\n",
       " 'notified': 247,\n",
       " 'damp': 297,\n",
       " 'interview': 76,\n",
       " 'heinous': 20,\n",
       " 'dane': 255,\n",
       " 'ethical': 211,\n",
       " 'travels': 292,\n",
       " 'liver': 4429,\n",
       " 'amout': 34,\n",
       " 'characterize': 32,\n",
       " 'prove': 470,\n",
       " 'youth': 406,\n",
       " 'understandably': 49,\n",
       " 'samu': 28,\n",
       " 'carbquik': 32,\n",
       " 'deetz': 42,\n",
       " 'embossed': 34,\n",
       " 'cubic': 23,\n",
       " 'betcha': 63,\n",
       " 'shunt': 25,\n",
       " 'affectionately': 21,\n",
       " 'chin': 117,\n",
       " 'aaa': 58,\n",
       " 'itched': 31,\n",
       " 'mcvitie': 57,\n",
       " 'tumbler': 72,\n",
       " 'original': 10199,\n",
       " 'autolyzed': 84,\n",
       " 'displayed': 215,\n",
       " 'izzy': 77,\n",
       " 'zp': 114,\n",
       " 'johnny': 84,\n",
       " 'kibbles': 656,\n",
       " 'clogged': 181,\n",
       " '1990': 72,\n",
       " 'emanating': 26,\n",
       " 'stinking': 67,\n",
       " 'zoe': 368,\n",
       " 'leisure': 43,\n",
       " 'finish': 5689,\n",
       " 'bon': 273,\n",
       " 'frankie': 31,\n",
       " 'contracting': 29,\n",
       " 'yang': 30,\n",
       " 'hudson': 26,\n",
       " 'distinguished': 41,\n",
       " 'insult': 154,\n",
       " 'laugh': 423,\n",
       " 'founded': 140,\n",
       " 'smoke': 1563,\n",
       " '~1': 37,\n",
       " 'rapidly': 279,\n",
       " 'lifespans': 129,\n",
       " 'charms': 212,\n",
       " 'aggressive': 516,\n",
       " 'padding': 235,\n",
       " 'glands': 94,\n",
       " 'confit': 25,\n",
       " '050': 21,\n",
       " 'monitor': 310,\n",
       " 'dabur': 24,\n",
       " 'terrifically': 22,\n",
       " 'virtually': 928,\n",
       " 'soledad': 44,\n",
       " 'pixy': 29,\n",
       " 'serums': 29,\n",
       " 'mn': 101,\n",
       " 'evanger': 34,\n",
       " 'promo': 117,\n",
       " 'carmine': 38,\n",
       " 'refresh': 118,\n",
       " 'unaware': 164,\n",
       " 'fiasco': 31,\n",
       " 'subtler': 34,\n",
       " 'sunsweet': 31,\n",
       " 'complications': 78,\n",
       " 'effectively': 292,\n",
       " 'handing': 162,\n",
       " 'disorders': 264,\n",
       " 'entrails': 30,\n",
       " 'ace': 82,\n",
       " 'collace': 20,\n",
       " 'unused': 347,\n",
       " 'forking': 30,\n",
       " 'rebate': 33,\n",
       " 'limes': 213,\n",
       " 'sincere': 41,\n",
       " 'haired': 155,\n",
       " 'os': 158,\n",
       " 'grocery': 21280,\n",
       " 'ladybug': 40,\n",
       " 'feared': 109,\n",
       " 'choir': 27,\n",
       " 'decafe': 132,\n",
       " 'sensation': 759,\n",
       " 'shun': 36,\n",
       " 'sprint': 34,\n",
       " 'tenderness': 99,\n",
       " 'cuts': 928,\n",
       " 'odor': 2757,\n",
       " 'disorder': 309,\n",
       " 'b001chfudc': 38,\n",
       " 'mmm': 678,\n",
       " 'himilayan': 24,\n",
       " 'started': 15617,\n",
       " 'mg': 2658,\n",
       " 'intestinal': 498,\n",
       " 'jimmies': 285,\n",
       " 'choice': 11226,\n",
       " 'dissapointment': 110,\n",
       " 'richly': 98,\n",
       " 'oxidant': 162,\n",
       " 'tots': 231,\n",
       " 'ward': 127,\n",
       " 'oyster': 230,\n",
       " 'tucker': 27,\n",
       " 'ski': 52,\n",
       " 'paradise': 131,\n",
       " 'rig': 26,\n",
       " 'od': 102,\n",
       " 'wafting': 50,\n",
       " 'usa': 3585,\n",
       " 'crunch': 7868,\n",
       " 'surely': 755,\n",
       " 'tahitian': 111,\n",
       " 'mandated': 22,\n",
       " 'fisherman': 36,\n",
       " 'shows': 1904,\n",
       " 'pads': 183,\n",
       " 'observations': 98,\n",
       " 'drastic': 125,\n",
       " 'thirsty': 484,\n",
       " 'twitter': 32,\n",
       " 'peices': 113,\n",
       " 'conducted': 103,\n",
       " 'needs': 8610,\n",
       " 'headache': 683,\n",
       " 'helpers': 33,\n",
       " 'promoting': 187,\n",
       " 'squish': 80,\n",
       " 'nut': 6813,\n",
       " 'heating': 760,\n",
       " 'font': 58,\n",
       " 'draws': 85,\n",
       " 'indonesian': 102,\n",
       " 'vegies': 128,\n",
       " 'convinient': 33,\n",
       " 'operate': 159,\n",
       " 'product': 187840,\n",
       " 'ahi': 66,\n",
       " 'salad': 6240,\n",
       " 'dimensional': 96,\n",
       " 'kikkoman': 168,\n",
       " 'anxiously': 126,\n",
       " 'epoxy': 23,\n",
       " 'considerable': 312,\n",
       " 'surviving': 51,\n",
       " 'heathy': 81,\n",
       " 'treatment': 794,\n",
       " 'via': 3776,\n",
       " 'ethically': 160,\n",
       " 'bent': 270,\n",
       " 'park': 656,\n",
       " 'smear': 170,\n",
       " 'kippered': 34,\n",
       " 'blogger': 40,\n",
       " 'absolutly': 146,\n",
       " 'ms': 605,\n",
       " 'zotz': 64,\n",
       " 'dressing': 4026,\n",
       " 'proplan': 52,\n",
       " 'cracked': 1055,\n",
       " 'supersaver': 106,\n",
       " 'airedale': 58,\n",
       " 'differance': 33,\n",
       " 'fringe': 23,\n",
       " 'gardening': 122,\n",
       " 'issuing': 23,\n",
       " 'h20': 52,\n",
       " 'aerogrow': 99,\n",
       " 'busy': 3064,\n",
       " '80mg': 60,\n",
       " 'party': 3839,\n",
       " 'presenting': 46,\n",
       " 'charger': 133,\n",
       " 'remaining': 1495,\n",
       " 'shrinkage': 20,\n",
       " 'mercken': 54,\n",
       " 'shipments': 944,\n",
       " 'appassionato': 25,\n",
       " 'territorial': 28,\n",
       " 'ltd': 28,\n",
       " 'defunct': 21,\n",
       " 'concert': 33,\n",
       " 'needle': 341,\n",
       " 'patients': 286,\n",
       " 'centers': 191,\n",
       " 'unpalatable': 203,\n",
       " 'plain': 10171,\n",
       " 'component': 421,\n",
       " 'winds': 76,\n",
       " 'nine': 908,\n",
       " 'baileys': 46,\n",
       " 'robbed': 47,\n",
       " 'speculation': 21,\n",
       " 'limbs': 27,\n",
       " 'pees': 24,\n",
       " 'patties': 447,\n",
       " 'dreamfield': 52,\n",
       " 'talk': 1168,\n",
       " 'aranciata': 23,\n",
       " 'targeted': 72,\n",
       " 'spotted': 230,\n",
       " 'digestives': 65,\n",
       " 'nm': 36,\n",
       " 'pancakes': 3748,\n",
       " 'everyday': 4375,\n",
       " 'cliche': 27,\n",
       " 'personally': 4546,\n",
       " 'reformulate': 33,\n",
       " 'mommy': 413,\n",
       " 'faq': 46,\n",
       " 'advertisment': 21,\n",
       " 'wool': 72,\n",
       " 'scarcely': 21,\n",
       " 'unedible': 77,\n",
       " 'grandchild': 35,\n",
       " 'anyway': 7202,\n",
       " 'eighties': 21,\n",
       " 'gail': 36,\n",
       " 'subjects': 40,\n",
       " 'wonderful': 29897,\n",
       " 'plentiful': 212,\n",
       " 'stinks': 495,\n",
       " 'thin': 6135,\n",
       " 'marathons': 55,\n",
       " 'lag': 23,\n",
       " 'property': 142,\n",
       " 'momma': 97,\n",
       " 'pulls': 180,\n",
       " 'dept': 92,\n",
       " 'mikels': 28,\n",
       " 'immediatly': 30,\n",
       " 'persists': 27,\n",
       " 'smore': 23,\n",
       " 'mislabeled': 100,\n",
       " 'panni': 31,\n",
       " 'minerals': 2055,\n",
       " 'mixtures': 146,\n",
       " 'finum': 23,\n",
       " '9g': 254,\n",
       " 'furthermore': 255,\n",
       " 'generated': 82,\n",
       " 'novel': 278,\n",
       " 'avoiding': 494,\n",
       " 'substitutes': 758,\n",
       " 'teeth': 11363,\n",
       " 'resides': 27,\n",
       " 'bullmastiff': 39,\n",
       " 'resembles': 354,\n",
       " 'sensible': 233,\n",
       " 'docked': 36,\n",
       " 'dudes': 36,\n",
       " 'staples': 305,\n",
       " 'margarita': 873,\n",
       " 'appreciating': 37,\n",
       " 'dagoba': 75,\n",
       " 'rbst': 24,\n",
       " 'cult': 68,\n",
       " 'purchasers': 42,\n",
       " 'abbey': 63,\n",
       " 'fantasic': 25,\n",
       " 'distance': 385,\n",
       " 'contented': 46,\n",
       " 'ungodly': 26,\n",
       " 'hiatus': 20,\n",
       " 'donating': 165,\n",
       " 'hips': 1361,\n",
       " 'homeade': 43,\n",
       " 'ecc': 38,\n",
       " 'firm': 1979,\n",
       " 'rot': 124,\n",
       " 'whitish': 68,\n",
       " 'distributed': 426,\n",
       " 'met': 1158,\n",
       " 'cookin': 28,\n",
       " 'marbled': 26,\n",
       " 'herdez': 38,\n",
       " 'clipping': 42,\n",
       " 'olives': 2249,\n",
       " 'hyperthyroidism': 27,\n",
       " 'material': 1185,\n",
       " 'superb': 1762,\n",
       " 'noooo': 34,\n",
       " 'soba': 198,\n",
       " 'adjustment': 184,\n",
       " 'almondina': 29,\n",
       " 'bisuifite': 20,\n",
       " 'slapping': 33,\n",
       " 'trip': 3318,\n",
       " 'starters': 231,\n",
       " 'stranger': 115,\n",
       " 'stem': 234,\n",
       " 'casino': 33,\n",
       " 'discuss': 111,\n",
       " 'swelling': 116,\n",
       " 'smacks': 80,\n",
       " 'ingesting': 294,\n",
       " 'rite': 162,\n",
       " 'screwing': 51,\n",
       " 'determine': 496,\n",
       " 'troubling': 41,\n",
       " 'brother': 1479,\n",
       " 'claiming': 316,\n",
       " 'unusually': 183,\n",
       " 'acquisition': 27,\n",
       " 'fillers': 2123,\n",
       " 'squeezy': 24,\n",
       " 'fakelime': 28,\n",
       " 'mushed': 48,\n",
       " 'two': 50257,\n",
       " 'housed': 26,\n",
       " 'revert': 59,\n",
       " 'throw': 5985,\n",
       " 'groomer': 124,\n",
       " 'disastrous': 25,\n",
       " 'comprises': 44,\n",
       " 'capresso': 112,\n",
       " 'restless': 141,\n",
       " 'disposed': 101,\n",
       " 'angled': 22,\n",
       " 'europeans': 40,\n",
       " 'canisters': 718,\n",
       " 'crying': 550,\n",
       " 'positive': 3093,\n",
       " 'bbq': 4736,\n",
       " 'child': 3770,\n",
       " 'prevented': 111,\n",
       " 'tuscan': 179,\n",
       " 'appetit': 116,\n",
       " 'locust': 25,\n",
       " 'bride': 62,\n",
       " 'jawbreaker': 26,\n",
       " 'warts': 28,\n",
       " 'suspicions': 31,\n",
       " 'bengal': 214,\n",
       " 'hydroponics': 21,\n",
       " 'injuries': 28,\n",
       " 'hygienist': 27,\n",
       " 'avaliable': 22,\n",
       " 'vacuuming': 21,\n",
       " 'reddenbacher': 35,\n",
       " 'isles': 43,\n",
       " 'searching': 2673,\n",
       " 'mesh': 434,\n",
       " 'canadian': 506,\n",
       " 'accompany': 153,\n",
       " 'poppin': 34,\n",
       " 'albertsons': 84,\n",
       " 'sourcing': 117,\n",
       " 'physically': 260,\n",
       " 'scrumptious': 650,\n",
       " 'm': 8719,\n",
       " 'imparts': 180,\n",
       " 'bicycle': 87,\n",
       " 'quality': 41132,\n",
       " 'vinaigrettes': 26,\n",
       " 'petits': 30,\n",
       " 'crafty': 32,\n",
       " 'gp': 16031,\n",
       " 'tuned': 64,\n",
       " 'undaunted': 20,\n",
       " 'modifying': 23,\n",
       " 'pastilles': 40,\n",
       " 'depression': 219,\n",
       " 'linguini': 44,\n",
       " 'quit': 1340,\n",
       " 'disclosing': 45,\n",
       " 'makhani': 30,\n",
       " 'sneaks': 93,\n",
       " 'calendar': 36,\n",
       " 'unrecognizable': 77,\n",
       " 'assuredly': 26,\n",
       " 'ore': 24,\n",
       " 'critic': 92,\n",
       " 'trailer': 54,\n",
       " 'snobby': 42,\n",
       " 'toniq': 21,\n",
       " 'creeps': 35,\n",
       " 'cleveland': 44,\n",
       " 'measurable': 24,\n",
       " 'triscuits': 87,\n",
       " 'dense': 1763,\n",
       " 'performing': 55,\n",
       " 'recycles': 21,\n",
       " 'boasts': 169,\n",
       " 'batman': 140,\n",
       " '27th': 28,\n",
       " 'lone': 62,\n",
       " 'winston': 20,\n",
       " 'fighting': 399,\n",
       " 'supplying': 84,\n",
       " 'defect': 139,\n",
       " 'whey': 1482,\n",
       " 'importing': 65,\n",
       " 'allowing': 589,\n",
       " 'toddy': 95,\n",
       " 'heathful': 32,\n",
       " 'doa': 23,\n",
       " 'bathed': 57,\n",
       " 'subway': 97,\n",
       " 'debated': 79,\n",
       " 'c': 5436,\n",
       " 'frequent': 905,\n",
       " 'refund': 2362,\n",
       " 'heighten': 25,\n",
       " 'medications': 773,\n",
       " 'reminiscing': 31,\n",
       " 'dill': 549,\n",
       " 'using': 31746,\n",
       " 'adding': 8189,\n",
       " 'nile': 34,\n",
       " 'mountian': 40,\n",
       " 'oolong': 1410,\n",
       " 'discourage': 115,\n",
       " 'tarrazu': 87,\n",
       " 'sounding': 149,\n",
       " 'stinkers': 29,\n",
       " 'convience': 110,\n",
       " 'cartons': 513,\n",
       " 'cuban': 132,\n",
       " 'dementia': 43,\n",
       " 'crowd': 515,\n",
       " 'itches': 30,\n",
       " 'ave': 40,\n",
       " 'educate': 93,\n",
       " 'bentley': 173,\n",
       " 'stirs': 49,\n",
       " 'pasteurized': 183,\n",
       " 'succumbed': 20,\n",
       " 'sticky': 3399,\n",
       " 'oatmeal': 10806,\n",
       " 'god': 1826,\n",
       " 'mudslide': 167,\n",
       " 'snackmasters': 112,\n",
       " 'pricing': 1997,\n",
       " 'guarding': 26,\n",
       " 'jonesing': 50,\n",
       " 'packet': 7165,\n",
       " 'bugged': 25,\n",
       " 'flat': 2686,\n",
       " 'notice': 5389,\n",
       " 'honees': 97,\n",
       " 'hara': 75,\n",
       " 'shopped': 333,\n",
       " '1519': 44,\n",
       " 'inspecting': 50,\n",
       " 'cosmetics': 73,\n",
       " 'warmer': 303,\n",
       " '280': 86,\n",
       " 'toned': 108,\n",
       " 'establishment': 38,\n",
       " 'kickstarter': 20,\n",
       " 'suzie': 26,\n",
       " 'stronger': 5655,\n",
       " 'mmmmmm': 394,\n",
       " 'sufferer': 63,\n",
       " '20gum': 25,\n",
       " 'noon': 231,\n",
       " 'greg': 35,\n",
       " 'monopoly': 50,\n",
       " 'filing': 42,\n",
       " 'improves': 386,\n",
       " 'raspberries': 614,\n",
       " 'employed': 56,\n",
       " 'roller': 655,\n",
       " 'rolling': 591,\n",
       " 'adores': 389,\n",
       " 'hon': 34,\n",
       " 'averaging': 47,\n",
       " 'downgraded': 48,\n",
       " 'fizzies': 89,\n",
       " 'footed': 49,\n",
       " 'migrains': 21,\n",
       " 'samosas': 26,\n",
       " 'doctoring': 121,\n",
       " 'energizer': 66,\n",
       " 'sooooo': 723,\n",
       " 'haley': 47,\n",
       " 'signing': 178,\n",
       " 'jasmine': 2572,\n",
       " 'puff': 494,\n",
       " 'emphasized': 27,\n",
       " 'duty': 402,\n",
       " 'related': 902,\n",
       " 'greyish': 28,\n",
       " 'glee': 91,\n",
       " 'scorch': 37,\n",
       " 'gizzards': 22,\n",
       " 'wards': 24,\n",
       " 'imitation': 438,\n",
       " 'chachere': 45,\n",
       " 'theo': 97,\n",
       " 'frankly': 1027,\n",
       " 'potful': 36,\n",
       " 'concerning': 298,\n",
       " 'metabolism': 680,\n",
       " 'studies': 784,\n",
       " 'choked': 355,\n",
       " 'serious': 2325,\n",
       " 'yankee': 62,\n",
       " 'dietitian': 47,\n",
       " 'arthritis': 861,\n",
       " 'crips': 26,\n",
       " 'ea': 166,\n",
       " 'murky': 122,\n",
       " 'sewing': 22,\n",
       " 'marinara': 352,\n",
       " 'tantalizing': 109,\n",
       " 'goldendoodle': 103,\n",
       " 'reactive': 47,\n",
       " 'clothes': 442,\n",
       " 'uncomfortably': 34,\n",
       " 'salesperson': 20,\n",
       " 'pleasure': 1647,\n",
       " 'tempura': 36,\n",
       " 'leukemia': 35,\n",
       " 'finals': 57,\n",
       " 'fillets': 223,\n",
       " '100': 11983,\n",
       " 'degradable': 28,\n",
       " 'commonly': 443,\n",
       " 'hund': 37,\n",
       " 'le': 237,\n",
       " 'dramatically': 339,\n",
       " 'lesson': 578,\n",
       " 'pilling': 75,\n",
       " 'cheerfully': 24,\n",
       " 'tip': 1456,\n",
       " 'fairly': 5001,\n",
       " 'owe': 244,\n",
       " 'palmer': 96,\n",
       " 'persons': 163,\n",
       " 'devouring': 176,\n",
       " 'cronic': 28,\n",
       " 'clubs': 116,\n",
       " '9s': 29,\n",
       " 'disappoint': 1096,\n",
       " 'chore': 280,\n",
       " 'students': 410,\n",
       " 'unit': 1306,\n",
       " 'unchewable': 41,\n",
       " 'splurge': 435,\n",
       " 'warming': 439,\n",
       " 'mines': 37,\n",
       " 'pic': 145,\n",
       " 'swollen': 122,\n",
       " 'lists': 801,\n",
       " 'cats': 31772,\n",
       " 'flaver': 117,\n",
       " 'rosehip': 80,\n",
       " 'tense': 59,\n",
       " 'locks': 118,\n",
       " 'dedicated': 369,\n",
       " 'multi': 1445,\n",
       " 'harden': 209,\n",
       " 'zinfandel': 33,\n",
       " 'pouts': 23,\n",
       " 'handicapped': 23,\n",
       " 'does': 76722,\n",
       " 'nova': 38,\n",
       " 'scary': 470,\n",
       " 'constipate': 32,\n",
       " 'olestra': 21,\n",
       " 'ridiculously': 740,\n",
       " 'spoil': 501,\n",
       " 'sean': 21,\n",
       " 'motel': 109,\n",
       " 'ingedients': 32,\n",
       " '53': 185,\n",
       " 'binds': 20,\n",
       " 'crevices': 27,\n",
       " 'disintegrates': 33,\n",
       " 'numbers': 595,\n",
       " 'twists': 546,\n",
       " 'whereas': 814,\n",
       " 'elbow': 145,\n",
       " 'steering': 38,\n",
       " 'policy': 584,\n",
       " 'knee': 247,\n",
       " 'traced': 21,\n",
       " 'sanity': 115,\n",
       " 'reputation': 493,\n",
       " 'arrowhead': 289,\n",
       " 'blob': 150,\n",
       " 'scotland': 200,\n",
       " 'beats': 1768,\n",
       " 'barking': 137,\n",
       " 'naysayers': 32,\n",
       " 'caking': 79,\n",
       " 'belle': 39,\n",
       " 'authorities': 22,\n",
       " 'vitaspelt': 20,\n",
       " 'gawd': 27,\n",
       " 'sanka': 95,\n",
       " 'drove': 164,\n",
       " 'crispbreads': 73,\n",
       " 'silk': 711,\n",
       " 'layers': 495,\n",
       " 'apso': 94,\n",
       " 'nothin': 60,\n",
       " 'basset': 115,\n",
       " 'casein': 322,\n",
       " 'gullet': 27,\n",
       " 'doggies': 438,\n",
       " 'goodies': 759,\n",
       " 'fluffy': 1306,\n",
       " 'sideways': 77,\n",
       " 'sally': 47,\n",
       " 'extracts': 457,\n",
       " 'predicted': 39,\n",
       " 'exported': 27,\n",
       " 'alfalfa': 345,\n",
       " 'spaghetti': 2011,\n",
       " 'wilted': 61,\n",
       " 'acts': 582,\n",
       " 'fineness': 20,\n",
       " 'aztec': 45,\n",
       " 'fountains': 33,\n",
       " 'classmates': 49,\n",
       " 'sooth': 149,\n",
       " 'valrhona': 90,\n",
       " 'cookout': 43,\n",
       " 'together': 6848,\n",
       " 'prepping': 38,\n",
       " 'snapea': 112,\n",
       " 'yakisoba': 116,\n",
       " 'undrinkable': 524,\n",
       " 'kitties': 1806,\n",
       " 'otis': 54,\n",
       " 'paranoid': 90,\n",
       " 'commentary': 53,\n",
       " 'became': 3604,\n",
       " 'processing': 1102,\n",
       " 'detest': 79,\n",
       " 'regretfully': 27,\n",
       " 'episodes': 191,\n",
       " 'magazines': 47,\n",
       " 'rigid': 80,\n",
       " 'broccoli': 1650,\n",
       " 'emulsifier': 142,\n",
       " 'thanks': 10925,\n",
       " 'captivating': 52,\n",
       " 'maltose': 32,\n",
       " 'stopper': 65,\n",
       " 'seattles': 23,\n",
       " 'shoved': 75,\n",
       " 'preventive': 23,\n",
       " 'achy': 24,\n",
       " 'unwrapped': 141,\n",
       " 'scarf': 260,\n",
       " 'redmill': 24,\n",
       " 'balances': 160,\n",
       " 'lawry': 181,\n",
       " 'these': 247317,\n",
       " 'hg': 20,\n",
       " 'aggravating': 42,\n",
       " 'multitude': 120,\n",
       " 'tamped': 32,\n",
       " 'plunge': 228,\n",
       " 'orijen': 1101,\n",
       " 'overwhelm': 416,\n",
       " 'unmarked': 77,\n",
       " 'saut': 138,\n",
       " 'raman': 42,\n",
       " 'guarna': 22,\n",
       " 'nylabone': 690,\n",
       " 'tortured': 30,\n",
       " 'chix': 54,\n",
       " 'aztecs': 21,\n",
       " 'mealtime': 151,\n",
       " 'diseased': 76,\n",
       " 'group': 1090,\n",
       " 'closed': 1076,\n",
       " 'printer': 20,\n",
       " 'oxygen': 185,\n",
       " 'yawning': 35,\n",
       " 'bertie': 100,\n",
       " 'beverage': 4383,\n",
       " 'reputed': 39,\n",
       " '065': 22,\n",
       " 'frise': 131,\n",
       " 'surveys': 32,\n",
       " 'softness': 208,\n",
       " 'anthony': 31,\n",
       " 'timothy': 2300,\n",
       " 'fairway': 29,\n",
       " 'd3': 214,\n",
       " 'metamucel': 25,\n",
       " 'magical': 368,\n",
       " 'waste': 8123,\n",
       " 'dietary': 2052,\n",
       " 'forever': 1996,\n",
       " '49': 781,\n",
       " 'barleans': 23,\n",
       " 'gastronomic': 22,\n",
       " 'buttery': 1887,\n",
       " 'knuckles': 36,\n",
       " 'crf': 96,\n",
       " 'dispensers': 173,\n",
       " 'honeydew': 76,\n",
       " 'colored': 1535,\n",
       " 'oranges': 428,\n",
       " 'al': 819,\n",
       " 'yerba': 400,\n",
       " 'closes': 107,\n",
       " 'plasticy': 48,\n",
       " 'boring': 1359,\n",
       " 'realemon': 29,\n",
       " 'subsitute': 127,\n",
       " 'runnier': 84,\n",
       " 'jif': 249,\n",
       " 'kneaded': 28,\n",
       " 'pbj': 86,\n",
       " 'shard': 20,\n",
       " 'b': 4844,\n",
       " 'smelliest': 21,\n",
       " 'rhymes': 24,\n",
       " 'sockeye': 85,\n",
       " 'cycle': 871,\n",
       " 'carl': 30,\n",
       " 'edible': 2268,\n",
       " 'admission': 26,\n",
       " 'potatoe': 260,\n",
       " 'ga': 158,\n",
       " 'distracting': 74,\n",
       " 'rediscovered': 41,\n",
       " 'displeasing': 25,\n",
       " 'stirfry': 30,\n",
       " 'higly': 31,\n",
       " 'conditioners': 321,\n",
       " 'ravioli': 275,\n",
       " 'spouse': 264,\n",
       " 'flight': 323,\n",
       " 'lest': 107,\n",
       " 'garibaldi': 25,\n",
       " 'location': 494,\n",
       " 'azalea': 24,\n",
       " 'gentlease': 138,\n",
       " 'fails': 393,\n",
       " 'vis': 33,\n",
       " 'runts': 98,\n",
       " 'shabby': 116,\n",
       " 'undigested': 60,\n",
       " 'barr': 24,\n",
       " 'flatter': 96,\n",
       " 'concentrating': 25,\n",
       " 'evian': 52,\n",
       " 'goth': 41,\n",
       " 'american': 3732,\n",
       " 'monohydrochloride': 20,\n",
       " 'relax': 849,\n",
       " 'paragraph': 61,\n",
       " 'rudy': 70,\n",
       " 'youll': 41,\n",
       " 'stringent': 53,\n",
       " 'xlear': 72,\n",
       " 'stirred': 496,\n",
       " 'misshapen': 61,\n",
       " 'enticing': 290,\n",
       " 'appreciable': 35,\n",
       " 'acidity': 1075,\n",
       " 'midday': 139,\n",
       " 'dirt': 1004,\n",
       " 'ppl': 42,\n",
       " 'inactive': 44,\n",
       " 'diverticulitis': 24,\n",
       " 'coaxed': 32,\n",
       " 'bleah': 25,\n",
       " 'spooning': 48,\n",
       " 'passing': 478,\n",
       " 'heaviest': 32,\n",
       " 'opportunities': 75,\n",
       " 'uniform': 340,\n",
       " 'brightens': 31,\n",
       " 'warrior': 90,\n",
       " 'jerkies': 158,\n",
       " 'feat': 76,\n",
       " 'olio': 64,\n",
       " 'comparison': 2951,\n",
       " 'deficiencies': 48,\n",
       " 'moos': 41,\n",
       " 'hots': 111,\n",
       " 'noticeably': 541,\n",
       " 'shake': 4538,\n",
       " 'doable': 63,\n",
       " 'neater': 52,\n",
       " 'grapefruit': 1097,\n",
       " 'gatorade': 862,\n",
       " 'teething': 582,\n",
       " 'hanger': 23,\n",
       " 'research': 5407,\n",
       " 'revolving': 21,\n",
       " 'dm': 26,\n",
       " 'acacia': 76,\n",
       " 'mile': 666,\n",
       " 'tyson': 29,\n",
       " 'mannheim': 21,\n",
       " 'premier': 217,\n",
       " 'riley': 42,\n",
       " 'heartbroken': 50,\n",
       " 'closure': 148,\n",
       " 'rushes': 28,\n",
       " 'bottoms': 174,\n",
       " 'cbtl': 122,\n",
       " 'exaggeration': 138,\n",
       " 'clarifying': 32,\n",
       " 'prolonged': 109,\n",
       " 'shelved': 20,\n",
       " 'kindness': 69,\n",
       " 'meaningful': 34,\n",
       " 'sprung': 64,\n",
       " 'herpes': 42,\n",
       " 'sincerely': 306,\n",
       " '10am': 24,\n",
       " 'bumps': 183,\n",
       " 'freshner': 24,\n",
       " 'stalk': 89,\n",
       " '74': 156,\n",
       " 'louisiana': 438,\n",
       " 'purity': 177,\n",
       " 'rootbeer': 202,\n",
       " 'vegetarians': 437,\n",
       " 'flavour': 2518,\n",
       " 'essiac': 26,\n",
       " 'distribution': 328,\n",
       " 'seinfeld': 23,\n",
       " 'shorts': 49,\n",
       " 'heretofore': 20,\n",
       " 'soggy': 1312,\n",
       " 'ragged': 38,\n",
       " 'dissappointing': 51,\n",
       " 'grasses': 87,\n",
       " 'gushing': 21,\n",
       " 'dentist': 379,\n",
       " 'phew': 39,\n",
       " 'yums': 36,\n",
       " 'swish': 68,\n",
       " 'balm': 376,\n",
       " 'bissinger': 34,\n",
       " 'greedily': 44,\n",
       " 'fertilizer': 150,\n",
       " 'gray': 565,\n",
       " 'lacking': 1544,\n",
       " 'charlee': 107,\n",
       " 'joint': 725,\n",
       " 'diarreha': 27,\n",
       " 'stong': 141,\n",
       " 'chocolate': 67509,\n",
       " 'graphics': 58,\n",
       " 'retain': 409,\n",
       " 'pinches': 62,\n",
       " 'hermetically': 37,\n",
       " 'connections': 27,\n",
       " 'lugging': 149,\n",
       " 'dts': 26,\n",
       " 'ricotta': 74,\n",
       " 'definitive': 88,\n",
       " 'apprehensive': 193,\n",
       " 'trappist': 39,\n",
       " 'ac': 52,\n",
       " 'anorexic': 28,\n",
       " 'ballerina': 21,\n",
       " 'mmmmh': 25,\n",
       " 'injection': 40,\n",
       " 'tack': 44,\n",
       " 'paycheck': 46,\n",
       " 'evaporates': 34,\n",
       " 'dum': 55,\n",
       " 'th': 149,\n",
       " 'undeliverable': 23,\n",
       " 'deliciously': 634,\n",
       " 'cacoa': 27,\n",
       " 'swings': 85,\n",
       " 'yoohoo': 60,\n",
       " 'crumble': 1063,\n",
       " 'appreciate': 3389,\n",
       " 'gardeners': 20,\n",
       " 'powerfully': 39,\n",
       " 'resturant': 78,\n",
       " 'court': 84,\n",
       " 'gusto': 845,\n",
       " 'meant': 1969,\n",
       " ...}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(word_counts):\n",
    "    '''\n",
    "    Param: word_counts\n",
    "    Return: Vocab,vocab_to_int,int_to_vocab\n",
    "    '''\n",
    "    vocab = set(word_counts.keys())\n",
    "    \n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    \n",
    "    codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
    "    for i,code in enumerate(codes):\n",
    "        vocab_to_int[code] = i\n",
    "\n",
    "    for i,word in enumerate(vocab,4):\n",
    "        vocab_to_int[word] = i\n",
    "        \n",
    "    int_to_vocab = {i:word for word,i in vocab_to_int.items()}\n",
    "    return vocab,vocab_to_int,int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab,vocab_to_int,int_to_vocab = get_vocab(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22726 22730 22730\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab),len(vocab_to_int),len(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using pre-trained Conceptnet Numberbatch's Embeddings (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "def get_word_embeddings():\n",
    "    embeddings = {}\n",
    "    with open('./Datasets/embeddings/numberbatch-en-17.06.txt',encoding='utf-8') as em:\n",
    "        for embed in em:\n",
    "            em_line = embed.split(' ')\n",
    "            if len(em_line) > 2: # First line of file is no. of words , number of dimensions\n",
    "                word = em_line[0]\n",
    "                embedding = np.array(em_line[1:])\n",
    "                embeddings[word] = embedding\n",
    "    print('Word embeddings:', len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417194\n"
     ]
    }
   ],
   "source": [
    "CN_embeddings = get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_in_embeddings = [word for word in vocab if word not in CN_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words not in Ebeddings :  2759\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of words not in Ebeddings : \",len(not_in_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(int_to_vocab,embeddings,embedding_dim = 300):\n",
    "    '''\n",
    "    Params : int_to_vocab, embeddings, embedding_dim\n",
    "    Return : embedding matrix\n",
    "    '''\n",
    "    # Generating empty numpy matrix\n",
    "    embeding_matrix = np.zeros([len(vocab_to_int),embedding_dim])\n",
    "    embeding_matrix = embeding_matrix.astype(np.float32)\n",
    "    \n",
    "    #Generating random embeddings for words not in CN embeddings\n",
    "    for i,word in int_to_vocab.items():\n",
    "        if word in embeddings:\n",
    "            embeding_matrix[i] = embeddings[word]\n",
    "        else:\n",
    "            embeding_matrix[i] = np.array(np.random.normal(embedding_dim))\n",
    "    return embeding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeding_matrix = create_embedding_matrix(int_to_vocab,CN_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22730 22730\n"
     ]
    }
   ],
   "source": [
    "print(len(embeding_matrix),len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_source_target(sources, targets, vocab_to_int):\n",
    "    '''\n",
    "    Params : Sources, Targets, vocab_to_int\n",
    "    Return :encoded_sources, encoded_targets\n",
    "    '''\n",
    "    encoded_sources = []\n",
    "    encoded_targets = []\n",
    "    for source in sources:\n",
    "        encod_ent = []\n",
    "        for word in source.split():\n",
    "            if word in vocab_to_int:\n",
    "                encod_ent.append(vocab_to_int[word])\n",
    "            else:\n",
    "                encod_ent.append(vocab_to_int[\"<UNK>\"])\n",
    "        encoded_sources.append(encod_ent)\n",
    "    \n",
    "    for target in targets:\n",
    "        encod_ent = []\n",
    "        for word in target.split():\n",
    "            if word in vocab_to_int:\n",
    "                encod_ent.append(vocab_to_int[word])\n",
    "            else:\n",
    "                encod_ent.append(vocab_to_int[\"<UNK>\"])\n",
    "        encoded_targets.append(encod_ent)\n",
    "        \n",
    "    return encoded_sources, encoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_sources, encoded_targets = encode_source_target(text,summary,vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568412 568412\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_sources),len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building Input Placeholders\n",
    "def model_inputs():\n",
    "    '''\n",
    "    Returns : input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length\n",
    "    '''\n",
    "    input_ = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"inputs\")\n",
    "    target = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"target\")\n",
    "    \n",
    "    learning_rate = tf.placeholder(dtype=tf.float32,name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "    source_seq_length = tf.placeholder(dtype=tf.int32,shape=(None,),name=\"source_seq_length\")\n",
    "    target_seq_length = tf.placeholder(dtype=tf.int32,shape=(None,),name=\"target_seq_length\")\n",
    "    \n",
    "    max_target_seq_length = tf.reduce_max(target_seq_length,name=\"max_target_seq_length\")\n",
    "    return input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process decoder input\n",
    "def process_decoder_input(target_data,vocab_to_int,batch_size):\n",
    "    \n",
    "    strided_target = tf.strided_slice(target_data,(0,0),(batch_size,-1),(1,1))\n",
    "    go = tf.fill(value=vocab_to_int[\"<GO>\"],dims=(batch_size,1))\n",
    "    decoder_input = tf.concat((go,strided_target),axis=1)\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(embeded_rnn_input,rnn_size,keep_prob,num_layers,batch_size,source_sequence_length):\n",
    "\n",
    "    def get_lstm(rnn_size,keep_prob=0.7):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    #     forward lstm layer\n",
    "    cell_fw = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "\n",
    "    #     backward lstm layer\n",
    "    cell_bw = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    ((encoder_fw_outputs,\n",
    "              encoder_bw_outputs),\n",
    "             (encoder_fw_state,\n",
    "              encoder_bw_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,cell_bw=cell_bw,inputs=embeded_rnn_input,\n",
    "                                    sequence_length=source_sequence_length,dtype=tf.float32)\n",
    "                                                                     \n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "    \n",
    "    encoder_states = []\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        if isinstance(encoder_fw_state[i],tf.contrib.rnn.LSTMStateTuple):\n",
    "            encoder_state_c = tf.concat(values=(encoder_fw_state[i].c,encoder_bw_state[i].c),axis=1,name=\"encoder_fw_state_c\")\n",
    "            encoder_state_h = tf.concat(values=(encoder_fw_state[i].h,encoder_bw_state[i].h),axis=1,name=\"encoder_fw_state_h\")\n",
    "            encoder_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "        elif isinstance(encoder_fw_state[i], tf.Tensor):\n",
    "            encoder_state = tf.concat(values=(encoder_fw_state[i], encoder_bw_state[i]), axis=1, name='bidirectional_concat')\n",
    "        \n",
    "        encoder_states.append(encoder_state)\n",
    "    \n",
    "    encoder_states = tuple(encoder_states)\n",
    "    \n",
    "    return encoder_outputs,encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoder(dec_embed_input,decoder_cell,encoder_state, output_layer,\n",
    "                     target_sequence_length,max_target_length):\n",
    "    \n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input,target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    \n",
    "    (final_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,impute_finished=True,\n",
    "                                                     maximum_iterations=max_target_length)\n",
    "    \n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoder(embeddings,decoder_cell,encoder_state,output_layer,vocab_to_int,\n",
    "                      max_target_length,batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant(dtype=tf.int32,value=[vocab_to_int[\"<GO>\"]]),\n",
    "                           multiples=[batch_size],name=\"start_tokens\")\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                      start_tokens=start_tokens,\n",
    "                                                      end_token=vocab_to_int[\"<EOS>\"])\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    (final_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(decoder,impute_finished=True,\n",
    "                                                  maximum_iterations=max_target_length)\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_inputs,encoder_state,embedding,vocab_to_int,rnn_size,target_sequence_length,max_target_length,\n",
    "                   batch_size,num_layers):\n",
    "    \n",
    "    def get_lstm(rnn_size,keep_prob=0.7):\n",
    "        rnn_size = 2 * rnn_size\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    vocab_len = len(vocab_to_int)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_len,kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    \n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embedding,target_inputs)\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "        \n",
    "        training_logits = training_decoder(embed,decoder_cell,encoder_state,output_layer,\n",
    "                                         target_sequence_length,max_target_length)\n",
    "    \n",
    "        \n",
    "    with tf.variable_scope(\"decoding\",reuse=True):\n",
    "        \n",
    "        inference_logits = inference_decoder(embedding,decoder_cell,encoder_state,output_layer,vocab_to_int,\n",
    "                                          max_target_length,batch_size)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(source_input,target_input,embeding_matrix,vocab_to_int,source_sequence_length,\n",
    "                  target_sequence_length,max_target_length, rnn_size,keep_prob,num_layers,batch_size):\n",
    "    '''\n",
    "    Params : source_input,target_input,embeding_matrix,vocab_to_int,source_sequence_length,\n",
    "                  target_sequence_length,max_target_length, rnn_size,keep_prob,num_layers,batch_size\n",
    "    \n",
    "    Return : training_logits, inference_logits\n",
    "    '''\n",
    "    embedings = embeding_matrix\n",
    "    embed = tf.nn.embedding_lookup(embedings,source_input)\n",
    "    \n",
    "    encoder_output,encoder_states = encoding_layer(embed,rnn_size,keep_prob,num_layers,\n",
    "                                                   batch_size,source_sequence_length)\n",
    "    \n",
    "    training_logits, inference_logits = decoding_layer(target_input,encoder_states,embedings,\n",
    "                                                                vocab_to_int,rnn_size,target_sequence_length,\n",
    "                                                                max_target_length,batch_size,num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting the text and summary for better padding\n",
    "# sort based on length of length of text\n",
    "def sort_text_summary(texts,summaries):\n",
    "    text_length = [(i,text,len(text)) for i,text in enumerate(texts)]\n",
    "    text_length.sort(key=operator.itemgetter(2))\n",
    "    \n",
    "    sorted_text = [text for i,text,length in text_length]\n",
    "    sorted_summary = []\n",
    "    for i,text,length in text_length:\n",
    "        sorted_summary.append(summaries[i])\n",
    "    return sorted_text,sorted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_text, sorted_summary = sort_text_summary(encoded_sources,encoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568412"
      ]
     },
     "execution_count": 165,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Padding batches\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_length = max([len(sent) for sent in sentence_batch])\n",
    "    padded_sentences = []\n",
    "    for sent in sentence_batch:\n",
    "        sent_len = len(sent)\n",
    "        if len(sent) < max_length:\n",
    "            padded_sentences.append(sent + [vocab_to_int[\"<PAD>\"] for _ in range(max_length - sent_len)])\n",
    "        else:\n",
    "            padded_sentences.append(sent)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(encoded_sources, encoded_targets, batch_size):\n",
    "    \n",
    "    '''\n",
    "    Params : encoded_sources, encoded_targets, batch_size\n",
    "    Return : text_batch,summary_batch,source_seq_len,target_seq_len\n",
    "    '''\n",
    "    \n",
    "    sorted_text, sorted_summary = sort_text_summary(encoded_sources,encoded_targets)\n",
    "    \n",
    "    batch_count = len(sorted_text)//batch_size\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        text_batch = np.array(pad_sentence_batch(sorted_text[start:end]))\n",
    "        summary_batch = np.array(pad_sentence_batch(sorted_summary[start:end]))\n",
    "        \n",
    "        source_seq_len = [len(sent) for sent in text_batch]\n",
    "        target_seq_len = [len(sent) for sent in summary_batch]\n",
    "        \n",
    "        yield (text_batch,summary_batch,source_seq_len,target_seq_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparametrs\n",
    "epochs = 10\n",
    "batch_size = 512\n",
    "rnn_size = 256\n",
    "num_layers = 2\n",
    "learn_rate = 0.01\n",
    "keep_probability = 0.75\n",
    "\n",
    "#Model save path\n",
    "save_path = 'models/model'\n",
    "\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs   \n",
    "    input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length = model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(input_,target,embeding_matrix,vocab_to_int,source_seq_length,target_seq_length,\n",
    "                  max_target_seq_length,rnn_size,keep_probability,num_layers,batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_seq_length, max_target_seq_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits,target,masks)\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to training and validation sets (1 Batch for Validation and rest for Training)\n",
    "train_source = sorted_text[batch_size:]\n",
    "train_target = sorted_summary[batch_size:]\n",
    "valid_source = sorted_text[:batch_size]\n",
    "valid_target = sorted_summary[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "(valid_text_batch,valid_summary_batch,valid_source_seq_len,valid_target_seq_len) = next(get_batches(valid_source,valid_target,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 64\n"
     ]
    }
   ],
   "source": [
    "print(len(source_seq_len),len(target_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    5/8881 - Train Accuracy: 0.0031, Validation Accuracy: 0.0000, Loss: 9.8540\n",
      "Epoch   0 Batch   10/8881 - Train Accuracy: 0.0052, Validation Accuracy: 0.0035, Loss: 9.5644\n",
      "Epoch   0 Batch   15/8881 - Train Accuracy: 0.0141, Validation Accuracy: 0.0043, Loss: 9.1686\n",
      "Epoch   0 Batch   20/8881 - Train Accuracy: 0.0100, Validation Accuracy: 0.0043, Loss: 8.6641\n",
      "Epoch   0 Batch   25/8881 - Train Accuracy: 0.0156, Validation Accuracy: 0.0043, Loss: 8.3862\n",
      "Epoch   0 Batch   30/8881 - Train Accuracy: 0.0143, Validation Accuracy: 0.0043, Loss: 7.7328\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-210-554a9fc308e9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m                                   \u001b[0mkeep_prob\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mkeep_probability\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m                                   \u001b[0msource_seq_length\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0msource_seq_len\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m                                   \u001b[0mtarget_seq_length\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mtarget_seq_len\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m                               })\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    893\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 895\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    896\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1122\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mfeed_dict_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1123\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m-> 1124\u001b[0;31m                              feed_dict_tensor, options, run_metadata)\n\u001b[0m\u001b[1;32m   1125\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1126\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1319\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1320\u001b[0m       return self._do_call(_run_fn, self._session, feeds, fetches, targets,\n\u001b[0;32m-> 1321\u001b[0;31m                            options, run_metadata)\n\u001b[0m\u001b[1;32m   1322\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_prun_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeeds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetches\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1325\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1327\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1328\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1329\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/tensorflow/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1304\u001b[0m           return tf_session.TF_Run(session, options,\n\u001b[1;32m   1305\u001b[0m                                    \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1306\u001b[0;31m                                    status, run_metadata)\n\u001b[0m\u001b[1;32m   1307\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1308\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i,(text_batch,summary_batch,source_seq_len,target_seq_len) in enumerate(\n",
    "            get_batches(train_source,train_target,batch_size)):\n",
    "            \n",
    "           \n",
    "            _, loss = sess.run([train_op,cost],\n",
    "                              feed_dict={\n",
    "                                  input_ : text_batch,\n",
    "                                  target : summary_batch,\n",
    "                                  learning_rate:learn_rate,\n",
    "                                  keep_prob : keep_probability,\n",
    "                                  source_seq_length : source_seq_len,\n",
    "                                  target_seq_length : target_seq_len\n",
    "                              })\n",
    "            \n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                batch_train_logits = sess.run(inference_logits,\n",
    "                                             feed_dict={\n",
    "                                                 input_: text_batch,\n",
    "                                                 source_seq_length: source_seq_len,\n",
    "                                                 target_seq_length: target_seq_len,\n",
    "                                                 keep_prob: 1.0\n",
    "                                             })\n",
    "                \n",
    "                batch_valid_logits = sess.run(inference_logits,\n",
    "                                             feed_dict={\n",
    "                                                 input_: valid_text_batch,\n",
    "                                                 source_seq_length: valid_source_seq_len,\n",
    "                                                 target_seq_length: valid_target_seq_len,\n",
    "                                                 keep_prob: 1.0\n",
    "                                             })\n",
    "                \n",
    "                train_accuracy = get_accuracy(summary_batch,batch_train_logits)\n",
    "                valid_accuracy = get_accuracy(valid_summary_batch,batch_valid_logits)\n",
    "                \n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(sorted_text) // batch_size, train_accuracy, valid_accuracy, loss))\n",
    "                \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

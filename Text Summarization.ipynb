{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Imports \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from collections import Counter\n",
    "import operator\n",
    "from tensorflow.python.layers.core import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Good Quality Dog Food</td>\n",
       "      <td>I have bought several of the Vitality canned d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Not as Advertised</td>\n",
       "      <td>Product arrived labeled as Jumbo Salted Peanut...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"Delight\" says it all</td>\n",
       "      <td>This is a confection that has been around a fe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Cough Medicine</td>\n",
       "      <td>If you are looking for the secret ingredient i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Great taffy</td>\n",
       "      <td>Great taffy at a great price.  There was a wid...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Summary                                               Text\n",
       "0  Good Quality Dog Food  I have bought several of the Vitality canned d...\n",
       "1      Not as Advertised  Product arrived labeled as Jumbo Salted Peanut...\n",
       "2  \"Delight\" says it all  This is a confection that has been around a fe...\n",
       "3         Cough Medicine  If you are looking for the secret ingredient i...\n",
       "4            Great taffy  Great taffy at a great price.  There was a wid..."
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def read_reviews():\n",
    "    reviews = pd.read_csv(\"./Datasets/Reviews/Reviews.csv\")\n",
    "    reviews = reviews.dropna()\n",
    "    reviews = reviews.drop([\"Id\",\"ProductId\",\"UserId\",\"ProfileName\",\"HelpfulnessNumerator\",\"HelpfulnessDenominator\",\"Score\",\"Time\"]\n",
    "                 ,axis=1)\n",
    "    return reviews\n",
    "\n",
    "reviews = read_reviews()\n",
    "reviews.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Summary</th>\n",
       "      <th>Text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [Summary, Text]\n",
       "Index: []"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews[reviews.isnull().any(axis=1)] # All cells have values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Cleaning and Normalizing the text and summaries\n",
    "# Some contraction to expansion\n",
    "contractions = { \n",
    "\"ain't\": \"am not\",\n",
    "\"aren't\": \"are not\",\n",
    "\"can't\": \"cannot\",\n",
    "\"can't've\": \"cannot have\",\n",
    "\"'cause\": \"because\",\n",
    "\"could've\": \"could have\",\n",
    "\"couldn't\": \"could not\",\n",
    "\"couldn't've\": \"could not have\",\n",
    "\"didn't\": \"did not\",\n",
    "\"doesn't\": \"does not\",\n",
    "\"don't\": \"do not\",\n",
    "\"hadn't\": \"had not\",\n",
    "\"hadn't've\": \"had not have\",\n",
    "\"hasn't\": \"has not\",\n",
    "\"haven't\": \"have not\",\n",
    "\"he'd\": \"he would\",\n",
    "\"he'd've\": \"he would have\",\n",
    "\"he'll\": \"he will\",\n",
    "\"he's\": \"he is\",\n",
    "\"how'd\": \"how did\",\n",
    "\"how'll\": \"how will\",\n",
    "\"how's\": \"how is\",\n",
    "\"i'd\": \"i would\",\n",
    "\"i'll\": \"i will\",\n",
    "\"i'm\": \"i am\",\n",
    "\"i've\": \"i have\",\n",
    "\"isn't\": \"is not\",\n",
    "\"it'd\": \"it would\",\n",
    "\"it'll\": \"it will\",\n",
    "\"it's\": \"it is\",\n",
    "\"let's\": \"let us\",\n",
    "\"ma'am\": \"madam\",\n",
    "\"mayn't\": \"may not\",\n",
    "\"might've\": \"might have\",\n",
    "\"mightn't\": \"might not\",\n",
    "\"must've\": \"must have\",\n",
    "\"mustn't\": \"must not\",\n",
    "\"needn't\": \"need not\",\n",
    "\"oughtn't\": \"ought not\",\n",
    "\"shan't\": \"shall not\",\n",
    "\"sha'n't\": \"shall not\",\n",
    "\"she'd\": \"she would\",\n",
    "\"she'll\": \"she will\",\n",
    "\"she's\": \"she is\",\n",
    "\"should've\": \"should have\",\n",
    "\"shouldn't\": \"should not\",\n",
    "\"that'd\": \"that would\",\n",
    "\"that's\": \"that is\",\n",
    "\"there'd\": \"there had\",\n",
    "\"there's\": \"there is\",\n",
    "\"they'd\": \"they would\",\n",
    "\"they'll\": \"they will\",\n",
    "\"they're\": \"they are\",\n",
    "\"they've\": \"they have\",\n",
    "\"wasn't\": \"was not\",\n",
    "\"we'd\": \"we would\",\n",
    "\"we'll\": \"we will\",\n",
    "\"we're\": \"we are\",\n",
    "\"we've\": \"we have\",\n",
    "\"weren't\": \"were not\",\n",
    "\"what'll\": \"what will\",\n",
    "\"what're\": \"what are\",\n",
    "\"what's\": \"what is\",\n",
    "\"what've\": \"what have\",\n",
    "\"where'd\": \"where did\",\n",
    "\"where's\": \"where is\",\n",
    "\"who'll\": \"who will\",\n",
    "\"who's\": \"who is\",\n",
    "\"won't\": \"will not\",\n",
    "\"wouldn't\": \"would not\",\n",
    "\"you'd\": \"you would\",\n",
    "\"you'll\": \"you will\",\n",
    "\"you're\": \"you are\"\n",
    "}\n",
    "def normalization(review,remove_stopwords=False):\n",
    "    text = review.lower()\n",
    "    clean_text = []\n",
    "    for word in text.split():\n",
    "        if word in contractions:\n",
    "            clean_text.append(contractions[word])\n",
    "        else:\n",
    "            clean_text.append(word)\n",
    "    text = \" \".join(clean_text)\n",
    "    \n",
    "    # Format words and remove unwanted characters\n",
    "#     text = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "    text = re.sub(r'https', ' ', text)\n",
    "    text = re.sub(r'\\<a href', ' ', text)\n",
    "    text = re.sub(r'&amp;', '', text) \n",
    "    text = re.sub(r'[_\"\\-;%()|+&=*%.,!?:#$@\\[\\]/]', ' ', text)\n",
    "    text = re.sub(r'<br', ' ', text)\n",
    "    text = re.sub(r'/>', ' ', text)\n",
    "    text = re.sub(r'>', ' ', text)\n",
    "    text = re.sub(r'<', ' ', text)\n",
    "    text = re.sub(r'`', ' ', text)\n",
    "    text = re.sub(r'\\'', ' ', text)\n",
    "    \n",
    "    if remove_stopwords:\n",
    "        text = text.split()\n",
    "        stops = set(stopwords.words(\"english\"))\n",
    "        text = [w for w in text if not w in stops]\n",
    "        text = \" \".join(text)\n",
    "\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'   http   www amazon com gp product b000gwlugu  plocky s tortilla chips  red beans  n rice  7 ounce bag  pack of 12   a  i first tasted these chips while visiting relatives in ky  they are not available where i live  so i ordered them from amazon  wow  my friends and family are all addicted to them  the spicy flavor grabs you at the first bite  once a bag is open  it is gone '"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalization(reviews.Text[713])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def clean_reviews(texts):\n",
    "    return [normalization(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "summary = clean_reviews(reviews.Summary)\n",
    "text = clean_reviews(reviews.Text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None count in Summary  0\n",
      "None count in Text  0\n",
      "568412 568412\n"
     ]
    }
   ],
   "source": [
    "print(\"None count in Summary \",sum(x is None for x in summary))\n",
    "print(\"None count in Text \",sum(x is None for x in text))\n",
    "print(len(summary),len(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Counting the words in Text and summary and remove words having count less than threshold\n",
    "def get_word_count(texts,summaries,threshold=20):\n",
    "    '''\n",
    "    Params: Tests , Summaries ,threshold = 20\n",
    "    Return : word count dict\n",
    "    '''\n",
    "    tokens = []\n",
    "    for text in texts:\n",
    "        tokens.extend(text.split())\n",
    "    for summary in summaries:\n",
    "        tokens.extend(summary.split())\n",
    "    counts = Counter(tokens)\n",
    "    reduced_count = {word:i for word,i in counts.items() if i >= threshold}\n",
    "    return reduced_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "count = get_word_count(text,summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'101012': 24,\n",
       " 'quanities': 22,\n",
       " 'sift': 141,\n",
       " 'tisane': 43,\n",
       " 'cuisine': 1445,\n",
       " 'plantation': 189,\n",
       " 'smoothed': 78,\n",
       " 'pt': 41,\n",
       " 'mamma': 37,\n",
       " 'parmasean': 33,\n",
       " 'peta': 23,\n",
       " 'rehab': 84,\n",
       " 'id': 319,\n",
       " 'rack': 367,\n",
       " 'stashed': 113,\n",
       " 'actor': 37,\n",
       " 'flowery': 360,\n",
       " 'newsletter': 72,\n",
       " 'ginormous': 46,\n",
       " 'horrible': 6934,\n",
       " 'thrifty': 43,\n",
       " 'stiff': 497,\n",
       " 'nooks': 35,\n",
       " 'infections': 675,\n",
       " 'nose': 2675,\n",
       " 'ppl': 42,\n",
       " 'supporting': 317,\n",
       " 'roberts': 27,\n",
       " 'copied': 93,\n",
       " 'dong': 78,\n",
       " 'deceived': 128,\n",
       " '16lb': 21,\n",
       " 'cu': 38,\n",
       " 'epic': 123,\n",
       " 'unhelpful': 33,\n",
       " 'intervention': 71,\n",
       " 'borders': 114,\n",
       " 'cautious': 439,\n",
       " 'bumblebee': 57,\n",
       " '75': 1823,\n",
       " 'coopers': 37,\n",
       " 'cools': 257,\n",
       " 'devilish': 20,\n",
       " 'lactic': 141,\n",
       " 'microwaved': 312,\n",
       " '3lbs': 73,\n",
       " 'browned': 319,\n",
       " 'tourist': 55,\n",
       " 'values': 558,\n",
       " 'formaldehyde': 53,\n",
       " 'shrooms': 26,\n",
       " 'pickles': 581,\n",
       " 'mannitol': 30,\n",
       " 'optional': 281,\n",
       " 'hormones': 345,\n",
       " 'cured': 498,\n",
       " 'efficacious': 20,\n",
       " 'lengthwise': 37,\n",
       " 'introduced': 2363,\n",
       " 'chubby': 334,\n",
       " 'gatherings': 111,\n",
       " 'unadulterated': 152,\n",
       " 'craisins': 83,\n",
       " 'incidents': 64,\n",
       " 'thirsty': 484,\n",
       " 'wants': 3270,\n",
       " 'suave': 54,\n",
       " 'audio': 69,\n",
       " 'minimalist': 22,\n",
       " 'zp': 114,\n",
       " 'volcanic': 67,\n",
       " 'nam': 30,\n",
       " 'founded': 140,\n",
       " 'foamed': 34,\n",
       " 'aficionado': 211,\n",
       " 'patterns': 51,\n",
       " 'diane': 30,\n",
       " 'poisons': 102,\n",
       " 'decaf': 11252,\n",
       " 'pimples': 50,\n",
       " 'lotus': 271,\n",
       " 'acknowledgement': 27,\n",
       " 'aerogrow': 99,\n",
       " 'kneaded': 28,\n",
       " 'thee': 128,\n",
       " 'relevant': 83,\n",
       " 'cooperative': 40,\n",
       " 'suspects': 36,\n",
       " 'noone': 51,\n",
       " 'erupt': 22,\n",
       " 'physically': 260,\n",
       " 'knife': 879,\n",
       " 'dished': 20,\n",
       " 'hosting': 58,\n",
       " 'chair': 262,\n",
       " 'not': 632350,\n",
       " 'dehydrator': 88,\n",
       " 'mfg': 68,\n",
       " 'wring': 28,\n",
       " 'diapers': 153,\n",
       " 'scoured': 39,\n",
       " 'apposed': 23,\n",
       " 'appletinis': 27,\n",
       " '911': 34,\n",
       " 'asleep': 756,\n",
       " 'fronts': 25,\n",
       " 'beauties': 70,\n",
       " 'sugary': 2661,\n",
       " 'jd': 23,\n",
       " 'unobtrusive': 24,\n",
       " 'joints': 584,\n",
       " 'pizazz': 38,\n",
       " 'confidence': 417,\n",
       " '199': 39,\n",
       " 'grin': 108,\n",
       " 'freezed': 25,\n",
       " 'logistics': 29,\n",
       " 'individuals': 334,\n",
       " 'granddaughter': 409,\n",
       " 'sherry': 162,\n",
       " 'ale': 1054,\n",
       " 'marks': 396,\n",
       " 'unsupervised': 43,\n",
       " 'ganoderma': 57,\n",
       " 'rocked': 58,\n",
       " 'swore': 78,\n",
       " 'hormone': 182,\n",
       " 'nutrious': 36,\n",
       " 'lint': 23,\n",
       " 'carcinogenic': 121,\n",
       " 'dollars': 3416,\n",
       " 'dehydrated': 1259,\n",
       " 'smile': 819,\n",
       " 'blossomed': 25,\n",
       " 'albanese': 114,\n",
       " 'vulnerable': 30,\n",
       " 'seemingly': 348,\n",
       " 'necessities': 30,\n",
       " 'bella': 415,\n",
       " 'separates': 164,\n",
       " 'sayers': 22,\n",
       " 'shaking': 683,\n",
       " 'wk': 26,\n",
       " 'thousands': 351,\n",
       " 'likley': 25,\n",
       " 'wadded': 20,\n",
       " 'monophosphate': 34,\n",
       " 'inaccurate': 105,\n",
       " 'saag': 36,\n",
       " 'serotonin': 28,\n",
       " 'hbp': 21,\n",
       " 'disastrous': 25,\n",
       " 'cruel': 68,\n",
       " 'penne': 284,\n",
       " 'weaning': 110,\n",
       " 'jambalaya': 81,\n",
       " 'uniformity': 72,\n",
       " 'stabilizers': 25,\n",
       " 'freely': 183,\n",
       " 'equaled': 32,\n",
       " 'prayers': 81,\n",
       " 'dive': 139,\n",
       " 'cosmic': 196,\n",
       " 'canteen': 20,\n",
       " 'duo': 32,\n",
       " 'interact': 63,\n",
       " 'forgiving': 116,\n",
       " 'home': 20631,\n",
       " 'aforementioned': 176,\n",
       " '30mins': 20,\n",
       " 'fannings': 44,\n",
       " 'carmine': 38,\n",
       " 'smearing': 35,\n",
       " 'shrugged': 26,\n",
       " 'bha': 132,\n",
       " 'beetle': 52,\n",
       " 'zotz': 64,\n",
       " 'peterson': 392,\n",
       " 'amz': 58,\n",
       " 'dissolved': 540,\n",
       " 'seasonal': 576,\n",
       " 'ca': 660,\n",
       " 'abounds': 34,\n",
       " 'b0029xlh4y': 39,\n",
       " 'toooo': 35,\n",
       " 'allowing': 589,\n",
       " 'cheapest': 1329,\n",
       " 'casei': 57,\n",
       " 'dominate': 130,\n",
       " 'believed': 365,\n",
       " 'caponata': 22,\n",
       " 'stimulated': 36,\n",
       " 'manufacturer': 3067,\n",
       " 'disguises': 20,\n",
       " 'gg': 45,\n",
       " 'maypo': 65,\n",
       " 'couldn': 210,\n",
       " 'hadnt': 28,\n",
       " 'thin': 6135,\n",
       " 'gifted': 135,\n",
       " 'ilk': 34,\n",
       " 'wrestle': 57,\n",
       " 'tripping': 52,\n",
       " 'upload': 58,\n",
       " 'shortcuts': 54,\n",
       " 'prepacked': 25,\n",
       " 'colon': 237,\n",
       " 'craved': 109,\n",
       " 'apologies': 64,\n",
       " 'floated': 54,\n",
       " 'cleanser': 282,\n",
       " 'generating': 31,\n",
       " 'waved': 20,\n",
       " 'ewwwww': 27,\n",
       " 'spraying': 152,\n",
       " 'mischief': 220,\n",
       " 'chilled': 927,\n",
       " 'treatie': 20,\n",
       " 'recycling': 267,\n",
       " 'limits': 282,\n",
       " 'offend': 80,\n",
       " 'hg': 20,\n",
       " 'stall': 40,\n",
       " 'phone': 742,\n",
       " 'respond': 391,\n",
       " 'clover': 361,\n",
       " 'strenght': 33,\n",
       " 'scottie': 73,\n",
       " 'eldest': 49,\n",
       " 'chock': 282,\n",
       " 'tooo': 62,\n",
       " '3yr': 43,\n",
       " 'awfully': 220,\n",
       " 'caster': 21,\n",
       " 'smokes': 64,\n",
       " '1oz': 307,\n",
       " 'acceptance': 54,\n",
       " 'ka': 95,\n",
       " 'fantasicakes': 24,\n",
       " 'stale': 7342,\n",
       " 'starbucks': 12667,\n",
       " 'spunky': 30,\n",
       " '35g': 78,\n",
       " 'blackcurrant': 217,\n",
       " 'muffins': 2044,\n",
       " 'boycotting': 43,\n",
       " 'albertson': 61,\n",
       " 'repellant': 47,\n",
       " 'shepards': 27,\n",
       " 'entrance': 59,\n",
       " 'nauseated': 99,\n",
       " 'maldon': 81,\n",
       " 'abundant': 288,\n",
       " '50c': 37,\n",
       " 'contaminant': 28,\n",
       " 'research': 5407,\n",
       " 'liquorice': 179,\n",
       " 'figgy': 28,\n",
       " 'billy': 48,\n",
       " 'mirin': 73,\n",
       " 'plastics': 144,\n",
       " 'jagged': 51,\n",
       " 'carbquik': 32,\n",
       " 'unsulfured': 31,\n",
       " 'loosened': 30,\n",
       " 'savior': 59,\n",
       " 'tjs': 24,\n",
       " 'distaste': 35,\n",
       " 'flowing': 170,\n",
       " 'yummi': 45,\n",
       " 'decaffeination': 53,\n",
       " 'link': 1567,\n",
       " 'revise': 60,\n",
       " 'sumac': 81,\n",
       " 'protected': 300,\n",
       " 'hangovers': 46,\n",
       " 'carbon': 344,\n",
       " 'perfectly': 5705,\n",
       " 'defeats': 266,\n",
       " 'nat': 55,\n",
       " 'robbery': 118,\n",
       " 'targets': 26,\n",
       " 'specified': 264,\n",
       " 'strewn': 21,\n",
       " 'stacked': 140,\n",
       " 'lifter': 28,\n",
       " 'mixtures': 146,\n",
       " 'rejected': 289,\n",
       " 'audience': 161,\n",
       " 'walden': 647,\n",
       " 'beige': 90,\n",
       " 'mueller': 25,\n",
       " 'guarenteed': 21,\n",
       " 'mgs': 48,\n",
       " 'mango': 4443,\n",
       " 'nations': 183,\n",
       " 'repeats': 55,\n",
       " 'condiment': 618,\n",
       " 'notches': 84,\n",
       " 'blanch': 23,\n",
       " 'lignans': 32,\n",
       " 'losing': 1183,\n",
       " 'bm': 88,\n",
       " 'racoons': 30,\n",
       " 'cheek': 72,\n",
       " 'matzo': 96,\n",
       " 'connoisseurs': 138,\n",
       " 'forks': 28,\n",
       " 'horribly': 476,\n",
       " 'teapots': 34,\n",
       " 'lupus': 34,\n",
       " 'cofffee': 51,\n",
       " 'formerly': 201,\n",
       " 'unexpected': 573,\n",
       " 'goooood': 69,\n",
       " 'exfoliation': 23,\n",
       " 'pretzels': 4622,\n",
       " 'primula': 30,\n",
       " 'enlarged': 48,\n",
       " 'survivor': 48,\n",
       " 'bitters': 353,\n",
       " 'mays': 74,\n",
       " 'specialize': 28,\n",
       " 'ck': 77,\n",
       " 'tetrapak': 25,\n",
       " 'komodo': 32,\n",
       " 'sickly': 396,\n",
       " '39g': 21,\n",
       " 'x1': 41,\n",
       " 'tartlets': 26,\n",
       " 'deli': 535,\n",
       " '340': 53,\n",
       " 'onions': 1754,\n",
       " 'saeco': 167,\n",
       " 'tenderness': 99,\n",
       " 'nori': 401,\n",
       " 'peg': 22,\n",
       " 'shameful': 57,\n",
       " 'cleveland': 44,\n",
       " 'thr': 44,\n",
       " 'begging': 552,\n",
       " 'inc': 292,\n",
       " 'lift': 759,\n",
       " 'gruel': 35,\n",
       " 'caffeine': 10059,\n",
       " 'coyotes': 53,\n",
       " 'surplus': 33,\n",
       " 'football': 196,\n",
       " 'southeastern': 31,\n",
       " 'pm': 395,\n",
       " 'pinpointed': 32,\n",
       " 'nevermind': 31,\n",
       " 'environments': 33,\n",
       " 'occasionaly': 20,\n",
       " 'existing': 150,\n",
       " 'unable': 1779,\n",
       " 'jealous': 110,\n",
       " 'b000g6mbua': 28,\n",
       " 'smartbones': 85,\n",
       " 'poker': 50,\n",
       " 'steven': 28,\n",
       " 'sprightly': 20,\n",
       " 'tao': 98,\n",
       " 'epa': 81,\n",
       " 'lunchtime': 168,\n",
       " 'girly': 56,\n",
       " 'dispensed': 131,\n",
       " 'revived': 24,\n",
       " 'detest': 79,\n",
       " 'abd': 26,\n",
       " 'weavers': 38,\n",
       " 'shoots': 130,\n",
       " 'brookstone': 25,\n",
       " 'strategy': 93,\n",
       " 'ready': 5416,\n",
       " 'seductive': 47,\n",
       " 'deeelish': 20,\n",
       " 'napkin': 227,\n",
       " 'popsicle': 46,\n",
       " 'films': 66,\n",
       " 'puddle': 75,\n",
       " 'tought': 28,\n",
       " 'suites': 28,\n",
       " 'probiotics': 509,\n",
       " 'tuxedo': 48,\n",
       " 'salvage': 87,\n",
       " 'unrefrigerated': 36,\n",
       " 'leash': 248,\n",
       " 'clear': 5699,\n",
       " 'gnu': 20,\n",
       " 'samson': 47,\n",
       " 'cheesburger': 23,\n",
       " 'consumable': 58,\n",
       " 'stickers': 82,\n",
       " 'gator': 31,\n",
       " 'loosing': 157,\n",
       " 'popped': 2416,\n",
       " 'xyla': 25,\n",
       " 'suprises': 20,\n",
       " 'overcharge': 20,\n",
       " 'modified': 827,\n",
       " 'counterpart': 167,\n",
       " 'relief': 1089,\n",
       " 'actually': 27488,\n",
       " 'ferret': 51,\n",
       " 'snubbed': 52,\n",
       " 'throw': 5985,\n",
       " 'sever': 30,\n",
       " 'extraneous': 32,\n",
       " 'sores': 179,\n",
       " 'fondants': 40,\n",
       " 'chat': 52,\n",
       " 'roommates': 69,\n",
       " 'hamilton': 166,\n",
       " 'custards': 45,\n",
       " '6x': 34,\n",
       " 'granules': 542,\n",
       " 'primrose': 31,\n",
       " 'slipping': 66,\n",
       " '34': 605,\n",
       " 'deviled': 97,\n",
       " 'projects': 121,\n",
       " 'foolishly': 54,\n",
       " 'arrangements': 49,\n",
       " 'distillation': 25,\n",
       " 'quenching': 233,\n",
       " 'puchased': 25,\n",
       " 'vegans': 364,\n",
       " 'frutose': 29,\n",
       " 'monitor': 310,\n",
       " 'hors': 48,\n",
       " 'dismay': 288,\n",
       " 'debris': 181,\n",
       " 'pricewise': 27,\n",
       " 'blurry': 22,\n",
       " 'sumptuous': 37,\n",
       " 'require': 1668,\n",
       " 'relocated': 37,\n",
       " 'exception': 1815,\n",
       " 'mild': 9929,\n",
       " 'diseases': 211,\n",
       " '0g': 691,\n",
       " '440': 46,\n",
       " 'unfairly': 22,\n",
       " 'except': 5492,\n",
       " 'saw': 8888,\n",
       " 'twilight': 22,\n",
       " 'celebrity': 83,\n",
       " 'cupboards': 133,\n",
       " 'cassis': 27,\n",
       " 'truth': 810,\n",
       " 'crepe': 79,\n",
       " 'dribble': 61,\n",
       " 'praise': 468,\n",
       " 'poland': 145,\n",
       " '10x': 94,\n",
       " 'overflow': 129,\n",
       " 'chipped': 158,\n",
       " 'weekday': 74,\n",
       " 'dysfunction': 22,\n",
       " 'icing': 1217,\n",
       " 'warped': 48,\n",
       " 'percentages': 86,\n",
       " 'lil': 480,\n",
       " 'sharkbanana': 61,\n",
       " 'auto': 2254,\n",
       " 'ledge': 21,\n",
       " 'cappuchino': 47,\n",
       " 'tortellini': 87,\n",
       " 'verbena': 88,\n",
       " 'gummy': 2866,\n",
       " 'suffers': 301,\n",
       " 'decaff': 266,\n",
       " 'bile': 120,\n",
       " 'refill': 866,\n",
       " 'softly': 54,\n",
       " 'wonderfulness': 27,\n",
       " 'shorten': 33,\n",
       " 'turn': 4556,\n",
       " 'level': 4208,\n",
       " 'adaptation': 20,\n",
       " 'bypassed': 23,\n",
       " 'eagerly': 444,\n",
       " 'yadda': 67,\n",
       " 'dog': 83544,\n",
       " 'everynight': 39,\n",
       " 'fascination': 26,\n",
       " 'mae': 156,\n",
       " 'overtones': 266,\n",
       " 'undamaged': 289,\n",
       " 'appears': 2014,\n",
       " 'continues': 910,\n",
       " 'complement': 517,\n",
       " 'snackwells': 62,\n",
       " 'rationed': 30,\n",
       " 'regularity': 121,\n",
       " 'cornucopia': 76,\n",
       " 'openly': 47,\n",
       " 'quinn': 46,\n",
       " 'complains': 131,\n",
       " 'irritation': 220,\n",
       " 'ray': 394,\n",
       " 'tangerine': 654,\n",
       " 'started': 15617,\n",
       " 'tendon': 34,\n",
       " 'repel': 60,\n",
       " 'flavourful': 138,\n",
       " 'bakes': 329,\n",
       " 'discs': 859,\n",
       " 'mueslis': 36,\n",
       " 'regrettably': 31,\n",
       " 'thinner': 975,\n",
       " 'rather': 13103,\n",
       " 'sambar': 27,\n",
       " 'shoulders': 430,\n",
       " 'espresso': 9322,\n",
       " 'ritz': 350,\n",
       " 'transferred': 170,\n",
       " 'distraction': 122,\n",
       " 'divorce': 26,\n",
       " 'centerpiece': 62,\n",
       " 'professor': 87,\n",
       " 'blueberry': 4456,\n",
       " 'outgrown': 35,\n",
       " 'perishable': 180,\n",
       " 'ofcourse': 47,\n",
       " 'dual': 73,\n",
       " 'quanity': 116,\n",
       " 'follower': 21,\n",
       " 'fairness': 236,\n",
       " 'bulb': 193,\n",
       " 'nasty': 4404,\n",
       " 'wednesday': 181,\n",
       " 'across': 4095,\n",
       " 'heave': 21,\n",
       " 'likening': 20,\n",
       " 'automated': 60,\n",
       " 'petroleum': 209,\n",
       " 'bitterly': 22,\n",
       " 'rig': 26,\n",
       " 'coarse': 739,\n",
       " 'paw': 792,\n",
       " 'munchies': 377,\n",
       " 'pursue': 43,\n",
       " 'prayer': 85,\n",
       " 'blair': 168,\n",
       " 'sulfites': 173,\n",
       " 'extensive': 309,\n",
       " 'artifically': 26,\n",
       " 'pyrenees': 56,\n",
       " 'surprising': 818,\n",
       " 'bisto': 82,\n",
       " 'suppliments': 26,\n",
       " '15mg': 87,\n",
       " 'most': 45448,\n",
       " 'bubbled': 32,\n",
       " 'brisket': 142,\n",
       " 'logic': 137,\n",
       " 'pointed': 551,\n",
       " 'snuggle': 37,\n",
       " 'oils': 3860,\n",
       " 'harmless': 138,\n",
       " 'peets': 158,\n",
       " 'forcefully': 24,\n",
       " 'scaled': 29,\n",
       " 'horse': 505,\n",
       " 'herbamare': 25,\n",
       " 'scaring': 23,\n",
       " 'np': 29,\n",
       " 'graduates': 72,\n",
       " 'marketplace': 189,\n",
       " 'comes': 18307,\n",
       " 'circling': 20,\n",
       " 'saturation': 21,\n",
       " 'lighten': 81,\n",
       " 'unwilling': 42,\n",
       " 'bagel': 460,\n",
       " 'colicky': 90,\n",
       " 'ethics': 57,\n",
       " 'diving': 39,\n",
       " 'yougurt': 45,\n",
       " 'alway': 93,\n",
       " 'rightfully': 29,\n",
       " 'demon': 24,\n",
       " 'climb': 125,\n",
       " 'tapenade': 61,\n",
       " 'stacy': 176,\n",
       " 'legal': 277,\n",
       " 'coral': 22,\n",
       " 'desiccant': 34,\n",
       " 'banjo': 27,\n",
       " 'unknowingly': 47,\n",
       " 'raises': 155,\n",
       " 'developing': 310,\n",
       " 'wisdom': 178,\n",
       " 'savannah': 92,\n",
       " 'else': 14135,\n",
       " 'erratic': 63,\n",
       " '454': 49,\n",
       " 'sections': 174,\n",
       " 'voodoo': 29,\n",
       " 'shuts': 22,\n",
       " 'disappointingly': 93,\n",
       " 'geena': 137,\n",
       " 'symptoms': 1372,\n",
       " 'heaven': 3020,\n",
       " 'anyday': 51,\n",
       " 'greats': 21,\n",
       " '15lb': 89,\n",
       " 'chamomile': 2352,\n",
       " 'readers': 108,\n",
       " 'kettlecorn': 32,\n",
       " 'remark': 64,\n",
       " 'vindaloo': 53,\n",
       " 'bison': 247,\n",
       " 'guts': 68,\n",
       " 'aim': 59,\n",
       " 'starting': 2357,\n",
       " 'regretting': 37,\n",
       " 'purists': 109,\n",
       " 'pic': 145,\n",
       " 'forthcoming': 50,\n",
       " 'creaminess': 300,\n",
       " 'linked': 350,\n",
       " 'rda': 1393,\n",
       " 'reservation': 191,\n",
       " 'rimadyl': 24,\n",
       " 'harmonious': 25,\n",
       " 'told': 7310,\n",
       " 'gaga': 48,\n",
       " 'adhered': 30,\n",
       " 'calories': 25678,\n",
       " 'fought': 112,\n",
       " 'rinsed': 591,\n",
       " 'press': 2199,\n",
       " 'strings': 316,\n",
       " 'antique': 48,\n",
       " 'breakfast': 16872,\n",
       " 'predominately': 26,\n",
       " 'stellar': 284,\n",
       " 'pad': 617,\n",
       " 'promote': 422,\n",
       " 'overriding': 36,\n",
       " '425': 40,\n",
       " 'bank': 680,\n",
       " 'mystery': 477,\n",
       " 'polished': 162,\n",
       " 'tt': 33,\n",
       " 'accidentally': 624,\n",
       " 'twinings': 1993,\n",
       " 'usage': 382,\n",
       " 'twitch': 40,\n",
       " 'design': 1892,\n",
       " 'microorganisms': 34,\n",
       " 'borne': 38,\n",
       " 'groomed': 51,\n",
       " 'behaved': 89,\n",
       " 'zd': 24,\n",
       " 'temper': 60,\n",
       " 'strengths': 107,\n",
       " 'hope': 8848,\n",
       " 'hormel': 364,\n",
       " 'absorption': 198,\n",
       " 'peppers': 2334,\n",
       " 'leaner': 56,\n",
       " 'weston': 45,\n",
       " 'ewwww': 37,\n",
       " 'gardetto': 26,\n",
       " 'observations': 98,\n",
       " 'dental': 2699,\n",
       " 'ghee': 474,\n",
       " 'inoffensive': 61,\n",
       " 'ago': 14614,\n",
       " 'circle': 230,\n",
       " 'originali': 20,\n",
       " '1999': 59,\n",
       " 'frog': 94,\n",
       " 'waiting': 2449,\n",
       " 'lavendar': 59,\n",
       " 'herbed': 29,\n",
       " 'chapped': 37,\n",
       " 'recognition': 78,\n",
       " 'flavoring': 5095,\n",
       " 'skewed': 32,\n",
       " 'stiffness': 73,\n",
       " 'candida': 149,\n",
       " 'slowed': 87,\n",
       " 'lease': 23,\n",
       " 'gob': 24,\n",
       " 'vomit': 859,\n",
       " 'bergamot': 1897,\n",
       " 'por': 61,\n",
       " 'nerd': 41,\n",
       " 'earnest': 35,\n",
       " 'tastes': 51916,\n",
       " 'garbage': 2612,\n",
       " 'biobags': 47,\n",
       " 'screams': 81,\n",
       " 'carnival': 121,\n",
       " 'sauvignon': 33,\n",
       " 'iodide': 68,\n",
       " 'unlike': 5486,\n",
       " 'examination': 49,\n",
       " 'disliked': 313,\n",
       " 'thirds': 215,\n",
       " 'disillusioned': 22,\n",
       " 'gorgeous': 481,\n",
       " 'popcorn': 18173,\n",
       " 'raiser': 31,\n",
       " 'mt': 242,\n",
       " 'clarification': 61,\n",
       " 'mahal': 22,\n",
       " 'hilarious': 219,\n",
       " 'reishi': 28,\n",
       " 'stock': 7779,\n",
       " 'fibromyalgia': 109,\n",
       " 'tense': 59,\n",
       " 'transitional': 33,\n",
       " 'surroundings': 24,\n",
       " 'differences': 577,\n",
       " 'conference': 95,\n",
       " 'sugared': 357,\n",
       " 'tries': 877,\n",
       " 'hopping': 58,\n",
       " 'bathe': 46,\n",
       " 'bodies': 470,\n",
       " 'bolivia': 34,\n",
       " 'blues': 315,\n",
       " 'knowing': 2683,\n",
       " 'quieter': 34,\n",
       " 'pushing': 370,\n",
       " 'cleared': 667,\n",
       " 'crocker': 649,\n",
       " 'milligrams': 112,\n",
       " 'shingles': 24,\n",
       " 'enduring': 39,\n",
       " 'feeds': 274,\n",
       " 'proflowers': 112,\n",
       " 'sabor': 29,\n",
       " 'feather': 76,\n",
       " 'daves': 35,\n",
       " 'stainless': 402,\n",
       " 'murray': 456,\n",
       " 'mowing': 34,\n",
       " 'border': 743,\n",
       " 'coals': 21,\n",
       " 'siblings': 106,\n",
       " 'yummiest': 156,\n",
       " 'ii': 295,\n",
       " 'brad': 23,\n",
       " 'externally': 36,\n",
       " 'approaching': 124,\n",
       " '1tbsp': 34,\n",
       " 'sugars': 2988,\n",
       " 'tendons': 127,\n",
       " 'bend': 258,\n",
       " 'kittie': 32,\n",
       " 'lumpfish': 28,\n",
       " 'westies': 119,\n",
       " 'satisfied': 5314,\n",
       " 'ying': 23,\n",
       " 'preheated': 49,\n",
       " 'preview': 20,\n",
       " 'fireball': 25,\n",
       " 'oxo': 62,\n",
       " 'quenched': 45,\n",
       " 'deteriorating': 22,\n",
       " 'toned': 108,\n",
       " 'reverse': 329,\n",
       " 'regulated': 132,\n",
       " 'gunk': 136,\n",
       " 'sterling': 33,\n",
       " 'mailed': 421,\n",
       " 'thins': 1035,\n",
       " 'conditioners': 321,\n",
       " 'probs': 21,\n",
       " 'floral': 1330,\n",
       " 'ay': 43,\n",
       " 'snag': 69,\n",
       " 'erhs': 20,\n",
       " 'shelfs': 33,\n",
       " 'peasy': 35,\n",
       " 'chapter': 25,\n",
       " 'staples': 305,\n",
       " 'elevation': 48,\n",
       " '100x': 37,\n",
       " 'neurotic': 32,\n",
       " 'poupon': 48,\n",
       " 'wiry': 45,\n",
       " 'induction': 37,\n",
       " 'aerated': 40,\n",
       " 'squishy': 135,\n",
       " 'johnny': 84,\n",
       " 'torte': 23,\n",
       " 'gj': 46,\n",
       " 'kopali': 37,\n",
       " 'lactation': 99,\n",
       " 'needlessly': 32,\n",
       " 'fondant': 770,\n",
       " 'rattled': 20,\n",
       " 'toledo': 21,\n",
       " 'dreading': 41,\n",
       " 'pony': 40,\n",
       " 'dimension': 162,\n",
       " 'taurine': 607,\n",
       " 'crouton': 50,\n",
       " 'slash': 28,\n",
       " 'cute': 2268,\n",
       " 'pinching': 36,\n",
       " 'clusters': 1365,\n",
       " 'kate': 32,\n",
       " 'differently': 717,\n",
       " 'microwave': 5891,\n",
       " 'artist': 68,\n",
       " 'caramels': 495,\n",
       " 'jury': 117,\n",
       " 'ltd': 28,\n",
       " 'jittery': 695,\n",
       " 'sky': 338,\n",
       " 'vegetable': 3700,\n",
       " 'refunded': 544,\n",
       " 'vacations': 59,\n",
       " 'switch': 4965,\n",
       " 'teething': 582,\n",
       " 'wow': 7490,\n",
       " 'rescue': 1507,\n",
       " 'locks': 118,\n",
       " 'sweetens': 191,\n",
       " 'diner': 208,\n",
       " 'hines': 103,\n",
       " 'attach': 122,\n",
       " 'pricier': 468,\n",
       " '108': 140,\n",
       " 'urged': 55,\n",
       " 'pasteurized': 183,\n",
       " 'jiff': 76,\n",
       " 'swanson': 161,\n",
       " 'premature': 73,\n",
       " 'might': 19904,\n",
       " 'prepares': 68,\n",
       " 'crystalline': 39,\n",
       " 'munch': 1020,\n",
       " 'aspertame': 138,\n",
       " 'hfc': 23,\n",
       " 'dobermans': 33,\n",
       " 'fanny': 22,\n",
       " 'poulet': 20,\n",
       " 'spoiling': 154,\n",
       " 'coton': 45,\n",
       " 'shapes': 623,\n",
       " 'gmos': 353,\n",
       " 'absorbing': 95,\n",
       " 'looser': 67,\n",
       " 'redeeming': 114,\n",
       " 'muddy': 229,\n",
       " 'slimjim': 47,\n",
       " 'jummy': 28,\n",
       " 'bengals': 54,\n",
       " 'train': 1121,\n",
       " 'cells': 554,\n",
       " 'adventures': 53,\n",
       " 'bay': 1688,\n",
       " 'crickets': 35,\n",
       " 'purrfect': 29,\n",
       " 'tan': 204,\n",
       " 'scallops': 112,\n",
       " 'artisana': 101,\n",
       " 'safer': 552,\n",
       " 'amazone': 25,\n",
       " 'whatsoever': 1135,\n",
       " 'seriousness': 63,\n",
       " 'another': 27364,\n",
       " 'money': 20050,\n",
       " 'bottarga': 22,\n",
       " 'shipments': 944,\n",
       " 'classroom': 63,\n",
       " 'bubba': 48,\n",
       " 'cleans': 449,\n",
       " 'tainted': 257,\n",
       " 'uniqueness': 42,\n",
       " 'hes': 96,\n",
       " 'pleasant': 6296,\n",
       " 'hook': 142,\n",
       " 'sealer': 45,\n",
       " 'singing': 175,\n",
       " 'refilling': 111,\n",
       " 'test': 4472,\n",
       " 'airy': 573,\n",
       " 'congeal': 21,\n",
       " 'sippin': 23,\n",
       " 'drippings': 67,\n",
       " 'oberto': 312,\n",
       " 'determined': 439,\n",
       " 'exclamation': 20,\n",
       " 'reports': 463,\n",
       " 'hiss': 26,\n",
       " 'firmly': 180,\n",
       " 'brie': 121,\n",
       " 'gf': 4364,\n",
       " 'unicorn': 26,\n",
       " 'dancing': 158,\n",
       " 'numbs': 22,\n",
       " 'royale': 28,\n",
       " 'borderline': 190,\n",
       " 'bonking': 34,\n",
       " 'flavorful': 9920,\n",
       " 'impacts': 101,\n",
       " 'elixir': 110,\n",
       " 'nicest': 117,\n",
       " 'fungusamongus': 25,\n",
       " 'evoo': 323,\n",
       " 'call': 5970,\n",
       " 'chill': 431,\n",
       " 'sparing': 20,\n",
       " 'pillows': 153,\n",
       " 'blind': 470,\n",
       " 'designs': 125,\n",
       " 'interestingly': 237,\n",
       " 'aimed': 77,\n",
       " 'professional': 740,\n",
       " 'claw': 87,\n",
       " 'getting': 21424,\n",
       " 'insipid': 110,\n",
       " 'anthon': 30,\n",
       " 'herbalist': 61,\n",
       " 'procure': 32,\n",
       " 'queen': 238,\n",
       " 'area': 5680,\n",
       " 'couscous': 703,\n",
       " 'emergency': 1026,\n",
       " 'jasmine': 2572,\n",
       " 'wealthy': 30,\n",
       " 'enzymes': 386,\n",
       " 'staleness': 108,\n",
       " 'popping': 1307,\n",
       " 'beachside': 29,\n",
       " 'left': 10289,\n",
       " 'sampled': 1022,\n",
       " 'con': 817,\n",
       " 'insistent': 35,\n",
       " '72': 486,\n",
       " 'mastiffs': 36,\n",
       " 'tropicana': 35,\n",
       " 'law': 1875,\n",
       " 'capsaicin': 95,\n",
       " 'di': 154,\n",
       " 'eboost': 131,\n",
       " 'boarding': 50,\n",
       " 'grumpy': 35,\n",
       " 'ahem': 58,\n",
       " 'rotting': 162,\n",
       " 'coons': 58,\n",
       " 'printing': 65,\n",
       " 'expanding': 115,\n",
       " 'whites': 379,\n",
       " 'ineffective': 85,\n",
       " 'breaking': 1260,\n",
       " 'roast': 16247,\n",
       " 'progressively': 47,\n",
       " '5lb': 400,\n",
       " 'approximate': 81,\n",
       " 'trauma': 31,\n",
       " 'reccommended': 60,\n",
       " 'way': 52920,\n",
       " 'ordeal': 71,\n",
       " 'advertized': 176,\n",
       " 'upset': 3134,\n",
       " 'fancy': 3198,\n",
       " 'socially': 151,\n",
       " 'depression': 219,\n",
       " 'lima': 102,\n",
       " 'crawl': 64,\n",
       " 'signature': 628,\n",
       " 'kracker': 48,\n",
       " 'rover': 49,\n",
       " 'granolas': 233,\n",
       " 'crosses': 25,\n",
       " 'fleeting': 31,\n",
       " 'nunaturals': 66,\n",
       " 'hardware': 144,\n",
       " 'crude': 717,\n",
       " 'kansas': 141,\n",
       " 'anymore': 6299,\n",
       " 'kix': 89,\n",
       " 'role': 350,\n",
       " 'accessible': 164,\n",
       " 'inedible': 912,\n",
       " 'alaea': 34,\n",
       " 'szeged': 26,\n",
       " 'constipation': 1023,\n",
       " 'jewell': 28,\n",
       " 'grasping': 22,\n",
       " 'deb': 39,\n",
       " 'eric': 66,\n",
       " 'everlast': 30,\n",
       " 'scooby': 39,\n",
       " 'celebration': 106,\n",
       " 'pros': 1618,\n",
       " 'savored': 157,\n",
       " 'starts': 1874,\n",
       " ...}"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_vocab(word_counts):\n",
    "    '''\n",
    "    Param: word_counts\n",
    "    Return: Vocab,vocab_to_int,int_to_vocab\n",
    "    '''\n",
    "    vocab = set(word_counts.keys())\n",
    "    \n",
    "    vocab_to_int = {}\n",
    "    int_to_vocab = {}\n",
    "    \n",
    "    codes = [\"<UNK>\",\"<PAD>\",\"<EOS>\",\"<GO>\"]\n",
    "    for i,code in enumerate(codes):\n",
    "        vocab_to_int[code] = i\n",
    "\n",
    "    for i,word in enumerate(vocab,4):\n",
    "        vocab_to_int[word] = i\n",
    "        \n",
    "    int_to_vocab = {i:word for word,i in vocab_to_int.items()}\n",
    "    return vocab,vocab_to_int,int_to_vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vocab,vocab_to_int,int_to_vocab = get_vocab(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22726 22730 22730\n"
     ]
    }
   ],
   "source": [
    "print(len(vocab),len(vocab_to_int),len(int_to_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Using pre-trained Conceptnet Numberbatch's Embeddings (https://github.com/commonsense/conceptnet-numberbatch)\n",
    "def get_word_embeddings():\n",
    "    embeddings = {}\n",
    "    with open('./Datasets/embeddings/numberbatch-en-17.06.txt',encoding='utf-8') as em:\n",
    "        for embed in em:\n",
    "            em_line = embed.split(' ')\n",
    "            if len(em_line) > 2: # First line of file is no. of words , number of dimensions\n",
    "                word = em_line[0]\n",
    "                embedding = np.array(em_line[1:])\n",
    "                embeddings[word] = embedding\n",
    "    print('Word embeddings:', len(embeddings))\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word embeddings: 417194\n"
     ]
    }
   ],
   "source": [
    "CN_embeddings = get_word_embeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "not_in_embeddings = [word for word in vocab if word not in CN_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No. of words not in Ebeddings :  2759\n"
     ]
    }
   ],
   "source": [
    "print(\"No. of words not in Ebeddings : \",len(not_in_embeddings))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_embedding_matrix(int_to_vocab,embeddings,embedding_dim = 300):\n",
    "    '''\n",
    "    Params : int_to_vocab, embeddings, embedding_dim\n",
    "    Return : embedding matrix\n",
    "    '''\n",
    "    # Generating empty numpy matrix\n",
    "    embeding_matrix = np.zeros([len(vocab_to_int),embedding_dim])\n",
    "    embeding_matrix = embeding_matrix.astype(np.float32)\n",
    "    \n",
    "    #Generating random embeddings for words not in CN embeddings\n",
    "    for i,word in int_to_vocab.items():\n",
    "        if word in embeddings:\n",
    "            embeding_matrix[i] = embeddings[word]\n",
    "        else:\n",
    "            embeding_matrix[i] = np.array(np.random.normal(embedding_dim))\n",
    "    return embeding_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeding_matrix = create_embedding_matrix(int_to_vocab,CN_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22730 22730\n"
     ]
    }
   ],
   "source": [
    "print(len(embeding_matrix),len(vocab_to_int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_source_target(sources, targets, vocab_to_int):\n",
    "    '''\n",
    "    Params : Sources, Targets, vocab_to_int\n",
    "    Return :encoded_sources, encoded_targets\n",
    "    '''\n",
    "    encoded_sources = []\n",
    "    encoded_targets = []\n",
    "    for source in sources:\n",
    "        encod_ent = []\n",
    "        for word in source.split():\n",
    "            if word in vocab_to_int:\n",
    "                encod_ent.append(vocab_to_int[word])\n",
    "            else:\n",
    "                encod_ent.append(vocab_to_int[\"<UNK>\"])\n",
    "        encoded_sources.append(encod_ent)\n",
    "    \n",
    "    for target in targets:\n",
    "        encod_ent = []\n",
    "        for word in target.split():\n",
    "            if word in vocab_to_int:\n",
    "                encod_ent.append(vocab_to_int[word])\n",
    "            else:\n",
    "                encod_ent.append(vocab_to_int[\"<UNK>\"])\n",
    "        encoded_targets.append(encod_ent)\n",
    "        \n",
    "    return encoded_sources, encoded_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "encoded_sources, encoded_targets = encode_source_target(text,summary,vocab_to_int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "568412 568412\n"
     ]
    }
   ],
   "source": [
    "print(len(encoded_sources),len(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Building Input Placeholders\n",
    "def model_inputs():\n",
    "    '''\n",
    "    Returns : input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length\n",
    "    '''\n",
    "    input_ = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"inputs\")\n",
    "    target = tf.placeholder(dtype=tf.int32,shape=(None,None),name=\"target\")\n",
    "    \n",
    "    learning_rate = tf.placeholder(dtype=tf.float32,name=\"learning_rate\")\n",
    "    keep_prob = tf.placeholder(dtype=tf.float32,name=\"keep_prob\")\n",
    "    \n",
    "    source_seq_length = tf.placeholder(dtype=tf.int32,shape=(None,),name=\"source_seq_length\")\n",
    "    target_seq_length = tf.placeholder(dtype=tf.int32,shape=(None,),name=\"target_seq_length\")\n",
    "    \n",
    "    max_target_seq_length = tf.reduce_max(target_seq_length,name=\"max_target_seq_length\")\n",
    "    return input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Process decoder input\n",
    "def process_decoder_input(target_data,vocab_to_int,batch_size):\n",
    "    \n",
    "    strided_target = tf.strided_slice(target_data,(0,0),(batch_size,-1),(1,1))\n",
    "    go = tf.fill(value=vocab_to_int[\"<GO>\"],dims=(batch_size,1))\n",
    "    decoder_input = tf.concat((go,strided_target),axis=1)\n",
    "    return decoder_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encoding_layer(embeded_rnn_input,rnn_size,keep_prob,num_layers,batch_size,source_sequence_length):\n",
    "\n",
    "    def get_lstm(rnn_size,keep_prob=0.7):\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    #     forward lstm layer\n",
    "    cell_fw = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "\n",
    "    #     backward lstm layer\n",
    "    cell_bw = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size,keep_prob) for _ in range(num_layers)])\n",
    "    \n",
    "    ((encoder_fw_outputs,\n",
    "              encoder_bw_outputs),\n",
    "             (encoder_fw_state,\n",
    "              encoder_bw_state)) = tf.nn.bidirectional_dynamic_rnn(cell_fw=cell_fw,cell_bw=cell_bw,inputs=embeded_rnn_input,\n",
    "                                    sequence_length=source_sequence_length,dtype=tf.float32)\n",
    "                                                                     \n",
    "    encoder_outputs = tf.concat((encoder_fw_outputs, encoder_bw_outputs), 2)\n",
    "    \n",
    "    encoder_states = []\n",
    "    \n",
    "    for i in range(num_layers):\n",
    "        if isinstance(encoder_fw_state[i],tf.contrib.rnn.LSTMStateTuple):\n",
    "            encoder_state_c = tf.concat(values=(encoder_fw_state[i].c,encoder_bw_state[i].c),axis=1,name=\"encoder_fw_state_c\")\n",
    "            encoder_state_h = tf.concat(values=(encoder_fw_state[i].h,encoder_bw_state[i].h),axis=1,name=\"encoder_fw_state_h\")\n",
    "            encoder_state = tf.contrib.rnn.LSTMStateTuple(c=encoder_state_c, h=encoder_state_h)\n",
    "        elif isinstance(encoder_fw_state[i], tf.Tensor):\n",
    "            encoder_state = tf.concat(values=(encoder_fw_state[i], encoder_bw_state[i]), axis=1, name='bidirectional_concat')\n",
    "        \n",
    "        encoder_states.append(encoder_state)\n",
    "    \n",
    "    encoder_states = tuple(encoder_states)\n",
    "    \n",
    "    return encoder_outputs,encoder_states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def training_decoder(dec_embed_input,decoder_cell,encoder_state, output_layer,\n",
    "                     target_sequence_length,max_target_length):\n",
    "    \n",
    "    \n",
    "    helper = tf.contrib.seq2seq.TrainingHelper(dec_embed_input,target_sequence_length)\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    \n",
    "    (final_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(decoder=decoder,impute_finished=True,\n",
    "                                                     maximum_iterations=max_target_length)\n",
    "    \n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def inference_decoder(embeddings,decoder_cell,encoder_state,output_layer,vocab_to_int,\n",
    "                      max_target_length,batch_size):\n",
    "    \n",
    "    start_tokens = tf.tile(tf.constant(dtype=tf.int32,value=[vocab_to_int[\"<GO>\"]]),\n",
    "                           multiples=[batch_size],name=\"start_tokens\")\n",
    "    \n",
    "    helper = tf.contrib.seq2seq.GreedyEmbeddingHelper(embeddings,\n",
    "                                                      start_tokens=start_tokens,\n",
    "                                                      end_token=vocab_to_int[\"<EOS>\"])\n",
    "    \n",
    "    decoder = tf.contrib.seq2seq.BasicDecoder(decoder_cell,helper,initial_state=encoder_state,\n",
    "                                              output_layer=output_layer)\n",
    "    \n",
    "    (final_outputs, final_state, final_sequence_lengths) = tf.contrib.seq2seq.dynamic_decode(decoder,impute_finished=True,\n",
    "                                                  maximum_iterations=max_target_length)\n",
    "    return final_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def decoding_layer(target_inputs,encoder_state,embedding,vocab_to_int,rnn_size,target_sequence_length,max_target_length,\n",
    "                   batch_size,num_layers):\n",
    "    \n",
    "    def get_lstm(rnn_size,keep_prob=0.7):\n",
    "        rnn_size = 2 * rnn_size\n",
    "        lstm = tf.contrib.rnn.BasicLSTMCell(rnn_size)\n",
    "        drop = tf.contrib.rnn.DropoutWrapper(lstm,input_keep_prob=keep_prob)\n",
    "        return drop\n",
    "    vocab_len = len(vocab_to_int)\n",
    "    decoder_cell = tf.contrib.rnn.MultiRNNCell([get_lstm(rnn_size) for _ in range(num_layers)])\n",
    "    output_layer = Dense(vocab_len,kernel_initializer=tf.truncated_normal_initializer(stddev=0.1))\n",
    "    \n",
    "    \n",
    "    embed = tf.nn.embedding_lookup(embedding,target_inputs)\n",
    "    \n",
    "    with tf.variable_scope(\"decoding\"):\n",
    "        \n",
    "        training_logits = training_decoder(embed,decoder_cell,encoder_state,output_layer,\n",
    "                                         target_sequence_length,max_target_length)\n",
    "    \n",
    "        \n",
    "    with tf.variable_scope(\"decoding\",reuse=True):\n",
    "        \n",
    "        inference_logits = inference_decoder(embedding,decoder_cell,encoder_state,output_layer,vocab_to_int,\n",
    "                                          max_target_length,batch_size)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def seq2seq_model(source_input,target_input,embeding_matrix,vocab_to_int,source_sequence_length,\n",
    "                  target_sequence_length,max_target_length, rnn_size,keep_prob,num_layers,batch_size):\n",
    "    '''\n",
    "    Params : source_input,target_input,embeding_matrix,vocab_to_int,source_sequence_length,\n",
    "                  target_sequence_length,max_target_length, rnn_size,keep_prob,num_layers,batch_size\n",
    "    \n",
    "    Return : training_logits, inference_logits\n",
    "    '''\n",
    "    embedings = embeding_matrix\n",
    "    embed = tf.nn.embedding_lookup(embedings,source_input)\n",
    "    \n",
    "    encoder_output,encoder_states = encoding_layer(embed,rnn_size,keep_prob,num_layers,\n",
    "                                                   batch_size,source_sequence_length)\n",
    "    \n",
    "    training_logits, inference_logits = decoding_layer(target_input,encoder_states,embedings,\n",
    "                                                                vocab_to_int,rnn_size,target_sequence_length,\n",
    "                                                                max_target_length,batch_size,num_layers)\n",
    "    \n",
    "    return training_logits, inference_logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Batching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sorting the text and summary for better padding\n",
    "# sort based on length of length of text\n",
    "def sort_text_summary(texts,summaries):\n",
    "    text_length = [(i,text,len(text)) for i,text in enumerate(texts)]\n",
    "    text_length.sort(key=operator.itemgetter(2))\n",
    "    \n",
    "    sorted_text = [text for i,text,length in text_length]\n",
    "    sorted_summary = []\n",
    "    for i,text,length in text_length:\n",
    "        sorted_summary.append(summaries[i])\n",
    "    return sorted_text,sorted_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sorted_text, sorted_summary = sort_text_summary(encoded_sources,encoded_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "568412"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sorted_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Padding batches\n",
    "def pad_sentence_batch(sentence_batch):\n",
    "    max_length = max([len(sent) for sent in sentence_batch])\n",
    "    padded_sentences = []\n",
    "    for sent in sentence_batch:\n",
    "        sent_len = len(sent)\n",
    "        if len(sent) < max_length:\n",
    "            padded_sentences.append(sent + [vocab_to_int[\"<PAD>\"] for _ in range(max_length - sent_len)])\n",
    "        else:\n",
    "            padded_sentences.append(sent)\n",
    "    return padded_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_batches(encoded_sources, encoded_targets, batch_size):\n",
    "    \n",
    "    '''\n",
    "    Params : encoded_sources, encoded_targets, batch_size\n",
    "    Return : text_batch,summary_batch,source_seq_len,target_seq_len\n",
    "    '''\n",
    "    \n",
    "    sorted_text, sorted_summary = sort_text_summary(encoded_sources,encoded_targets)\n",
    "    \n",
    "    batch_count = len(sorted_text)//batch_size\n",
    "    \n",
    "    for i in range(batch_count):\n",
    "        start = i * batch_size\n",
    "        end = start + batch_size\n",
    "        \n",
    "        text_batch = np.array(pad_sentence_batch(sorted_text[start:end]))\n",
    "        summary_batch = np.array(pad_sentence_batch(sorted_summary[start:end]))\n",
    "        \n",
    "        source_seq_len = [len(sent) for sent in text_batch]\n",
    "        target_seq_len = [len(sent) for sent in summary_batch]\n",
    "        \n",
    "        yield (text_batch,summary_batch,source_seq_len,target_seq_len)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hyperparametrs\n",
    "epochs = 3\n",
    "batch_size = 512\n",
    "rnn_size = 100\n",
    "num_layers = 3\n",
    "learn_rate = 0.01\n",
    "keep_probability = 0.75\n",
    "\n",
    "#Model save path\n",
    "save_path = 'models/model'\n",
    "\n",
    "display_step = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Build Graph\n",
    "\n",
    "train_graph = tf.Graph()\n",
    "with train_graph.as_default():\n",
    "    \n",
    "    # Load the model inputs   \n",
    "    input_,target,learning_rate,keep_prob,source_seq_length,target_seq_length,max_target_seq_length = model_inputs()\n",
    "    \n",
    "    # Create the training and inference logits\n",
    "    training_logits, inference_logits = seq2seq_model(input_,target,embeding_matrix,vocab_to_int,source_seq_length,target_seq_length,\n",
    "                  max_target_seq_length,rnn_size,keep_probability,num_layers,batch_size)\n",
    "    \n",
    "    # Create tensors for the training logits and inference logits\n",
    "    training_logits = tf.identity(training_logits.rnn_output, name='logits')\n",
    "    inference_logits = tf.identity(inference_logits.sample_id, name='predictions')\n",
    "    \n",
    "    masks = tf.sequence_mask(target_seq_length, max_target_seq_length, dtype=tf.float32, name='masks')\n",
    "\n",
    "    with tf.name_scope(\"optimization\"):\n",
    "        cost = tf.contrib.seq2seq.sequence_loss(training_logits,target,masks)\n",
    "        optimizer=tf.train.AdamOptimizer(learning_rate)\n",
    "        \n",
    "        # Gradient Clipping\n",
    "        gradients = optimizer.compute_gradients(cost)\n",
    "        capped_gradients = [(tf.clip_by_value(grad, -1., 1.), var) for grad, var in gradients if grad is not None]\n",
    "        train_op = optimizer.apply_gradients(capped_gradients)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Accuracy\n",
    "def get_accuracy(target, logits):\n",
    "    \"\"\"\n",
    "    Calculate accuracy\n",
    "    \"\"\"\n",
    "    max_seq = max(target.shape[1], logits.shape[1])\n",
    "    if max_seq - target.shape[1]:\n",
    "        target = np.pad(\n",
    "            target,\n",
    "            [(0,0),(0,max_seq - target.shape[1])],\n",
    "            'constant')\n",
    "    if max_seq - logits.shape[1]:\n",
    "        logits = np.pad(\n",
    "            logits,\n",
    "            [(0,0),(0,max_seq - logits.shape[1])],\n",
    "            'constant')\n",
    "\n",
    "    return np.mean(np.equal(target, logits))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data to training and validation sets (1 Batch for Validation and rest for Training)\n",
    "train_source = sorted_text[batch_size:]\n",
    "train_target = sorted_summary[batch_size:]\n",
    "valid_source = sorted_text[:batch_size]\n",
    "valid_target = sorted_summary[:batch_size]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(valid_text_batch,valid_summary_batch,valid_source_seq_len,valid_target_seq_len) = next(get_batches(valid_source,valid_target,batch_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "512 512\n"
     ]
    }
   ],
   "source": [
    "print(len(valid_source_seq_len),len(valid_target_seq_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch   0 Batch    5/1110 - Train Accuracy: 0.7544, Validation Accuracy: 0.7522, Loss: 2.0595\n",
      "Epoch   0 Batch   10/1110 - Train Accuracy: 0.8000, Validation Accuracy: 0.7522, Loss: 1.4508\n",
      "Epoch   0 Batch   15/1110 - Train Accuracy: 0.7388, Validation Accuracy: 0.7522, Loss: 1.7768\n",
      "Epoch   0 Batch   20/1110 - Train Accuracy: 0.5776, Validation Accuracy: 0.7258, Loss: 1.9066\n",
      "Epoch   0 Batch   25/1110 - Train Accuracy: 0.2160, Validation Accuracy: 0.5138, Loss: 1.4715\n",
      "Epoch   0 Batch   30/1110 - Train Accuracy: 0.5788, Validation Accuracy: 0.6944, Loss: 0.9707\n",
      "Epoch   0 Batch   35/1110 - Train Accuracy: 0.7399, Validation Accuracy: 0.7314, Loss: 1.0079\n",
      "Epoch   0 Batch   40/1110 - Train Accuracy: 0.8546, Validation Accuracy: 0.7539, Loss: 0.9211\n",
      "Epoch   0 Batch   45/1110 - Train Accuracy: 0.8494, Validation Accuracy: 0.7539, Loss: 0.9985\n",
      "Epoch   0 Batch   50/1110 - Train Accuracy: 0.8483, Validation Accuracy: 0.7539, Loss: 0.9754\n",
      "Epoch   0 Batch   55/1110 - Train Accuracy: 0.8666, Validation Accuracy: 0.7539, Loss: 0.8379\n",
      "Epoch   0 Batch   60/1110 - Train Accuracy: 0.8492, Validation Accuracy: 0.7539, Loss: 0.9403\n",
      "Epoch   0 Batch   65/1110 - Train Accuracy: 0.7548, Validation Accuracy: 0.7539, Loss: 1.5871\n",
      "Epoch   0 Batch   70/1110 - Train Accuracy: 0.7566, Validation Accuracy: 0.7539, Loss: 1.5764\n",
      "Epoch   0 Batch   75/1110 - Train Accuracy: 0.7782, Validation Accuracy: 0.7539, Loss: 1.3726\n",
      "Epoch   0 Batch   80/1110 - Train Accuracy: 0.8158, Validation Accuracy: 0.7539, Loss: 1.1749\n",
      "Epoch   0 Batch   85/1110 - Train Accuracy: 0.8363, Validation Accuracy: 0.7539, Loss: 1.0665\n",
      "Epoch   0 Batch   90/1110 - Train Accuracy: 0.7827, Validation Accuracy: 0.7539, Loss: 1.3634\n",
      "Epoch   0 Batch   95/1110 - Train Accuracy: 0.7389, Validation Accuracy: 0.7539, Loss: 1.6007\n",
      "Epoch   0 Batch  100/1110 - Train Accuracy: 0.8664, Validation Accuracy: 0.7539, Loss: 0.8570\n",
      "Epoch   0 Batch  105/1110 - Train Accuracy: 0.8755, Validation Accuracy: 0.7539, Loss: 0.7660\n",
      "Epoch   0 Batch  110/1110 - Train Accuracy: 0.7396, Validation Accuracy: 0.7539, Loss: 1.5817\n",
      "Epoch   0 Batch  115/1110 - Train Accuracy: 0.7755, Validation Accuracy: 0.7539, Loss: 1.3788\n",
      "Epoch   0 Batch  120/1110 - Train Accuracy: 0.7847, Validation Accuracy: 0.7539, Loss: 1.3288\n",
      "Epoch   0 Batch  125/1110 - Train Accuracy: 0.7357, Validation Accuracy: 0.7539, Loss: 1.6362\n",
      "Epoch   0 Batch  130/1110 - Train Accuracy: 0.7177, Validation Accuracy: 0.7539, Loss: 1.7406\n",
      "Epoch   0 Batch  135/1110 - Train Accuracy: 0.7053, Validation Accuracy: 0.7539, Loss: 1.7734\n",
      "Epoch   0 Batch  140/1110 - Train Accuracy: 0.7637, Validation Accuracy: 0.7539, Loss: 1.3875\n",
      "Epoch   0 Batch  145/1110 - Train Accuracy: 0.8728, Validation Accuracy: 0.7539, Loss: 0.7323\n",
      "Epoch   0 Batch  150/1110 - Train Accuracy: 0.7316, Validation Accuracy: 0.7539, Loss: 1.5576\n",
      "Epoch   0 Batch  155/1110 - Train Accuracy: 0.7920, Validation Accuracy: 0.7539, Loss: 1.2337\n",
      "Epoch   0 Batch  160/1110 - Train Accuracy: 0.8868, Validation Accuracy: 0.7539, Loss: 0.6674\n",
      "Epoch   0 Batch  165/1110 - Train Accuracy: 0.8879, Validation Accuracy: 0.7539, Loss: 0.6351\n",
      "Epoch   0 Batch  170/1110 - Train Accuracy: 0.8423, Validation Accuracy: 0.7539, Loss: 0.9197\n",
      "Epoch   0 Batch  175/1110 - Train Accuracy: 0.8572, Validation Accuracy: 0.7539, Loss: 0.8311\n",
      "Epoch   0 Batch  180/1110 - Train Accuracy: 0.8401, Validation Accuracy: 0.7539, Loss: 0.8755\n",
      "Epoch   0 Batch  185/1110 - Train Accuracy: 0.7535, Validation Accuracy: 0.7539, Loss: 1.3303\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=train_graph) as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    for epoch_i in range(epochs):\n",
    "        for batch_i,(text_batch,summary_batch,source_seq_len,target_seq_len) in enumerate(\n",
    "            get_batches(train_source,train_target,batch_size)):\n",
    "            \n",
    "           \n",
    "            _, loss = sess.run([train_op,cost],\n",
    "                              feed_dict={\n",
    "                                  input_ : text_batch,\n",
    "                                  target : summary_batch,\n",
    "                                  learning_rate:learn_rate,\n",
    "                                  keep_prob : keep_probability,\n",
    "                                  source_seq_length : source_seq_len,\n",
    "                                  target_seq_length : target_seq_len\n",
    "                              })\n",
    "            \n",
    "            if batch_i % display_step == 0 and batch_i > 0:\n",
    "                \n",
    "                batch_train_logits = sess.run(inference_logits,\n",
    "                                             feed_dict={\n",
    "                                                 input_: text_batch,\n",
    "                                                 source_seq_length: source_seq_len,\n",
    "                                                 target_seq_length: target_seq_len,\n",
    "                                                 keep_prob: 1.0\n",
    "                                             })\n",
    "                \n",
    "                batch_valid_logits = sess.run(inference_logits,\n",
    "                                             feed_dict={\n",
    "                                                 input_: valid_text_batch,\n",
    "                                                 source_seq_length: valid_source_seq_len,\n",
    "                                                 target_seq_length: valid_target_seq_len,\n",
    "                                                 keep_prob: 1.0\n",
    "                                             })\n",
    "                \n",
    "                train_accuracy = get_accuracy(summary_batch,batch_train_logits)\n",
    "                valid_accuracy = get_accuracy(valid_summary_batch,batch_valid_logits)\n",
    "                \n",
    "                print('Epoch {:>3} Batch {:>4}/{} - Train Accuracy: {:>6.4f}, Validation Accuracy: {:>6.4f}, Loss: {:>6.4f}'\n",
    "                      .format(epoch_i, batch_i, len(sorted_text) // batch_size, train_accuracy, valid_accuracy, loss))\n",
    "                \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    saver.save(sess, save_path)\n",
    "    print('Model Trained and Saved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
